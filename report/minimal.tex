\input ctustyle
\worktype [M/CZ]
\faculty {F3}
\department {Katedra kybernetiky}
\title {Minimální dokument}
\author {Jakub Podlaha}
\date {May 2014}
\abstractEN {This document is for testing purpose only.}
\abstractCZ {Tento dokument je pouze pro potřeby testování.}
\declaration {Prohlašuji, že jsem se neflákal.}
\makefront



\chap Introduction

During past few years the Web went through bigger or smaller revolutions. 

\begitems
  * WEB 2.0 and tag cloud
  * HTML5 and semantic tags
  * Smartphones, Tablets and mobile web everywhere, 
  * The run out of IPv4 addresses, nonexistent boom of IPv6, 
  * Cloud technologies and BigData, 
  * Bitcoin, Tor, anonymous internet, 
  * WikiLeaks, NSA, Heartbleed and secourity concerns
  * Google Knowledge Graph, Facebook Open Graph, ...
\enditems

That's only few examples of some of the biggest recent issues on the web in
general. We live in an age, where so little can mean so much. The enviroment
online is dramatically changing, mostly on a wave of some new, useful or
frightening technology. The Semantic Web technologies have been described,
standardized and implemented for several year now XXX and their tide is near,
though yet to come.

Semantic Web itself reates to several principals (and their implementation)
that allow users to add meaning to their data. This meaning brings not only a
standardized structure, but also posibility to query and reason on it. Once
given the structure, similar data can be also joined in a form of a bigger
clowd. This phenomena is called Linked Data. 

In this work we'd like to bring the Semantic Web technologies closer to users.
The approach is to implement tool to simplify the process of giving meaning to
(yet anonymous) data on a webpage, i.e. to bring structure and meaning into it. 



\sec Problem Statement and Motivation

Giving meaning, i.e. semantization of web pages gets more popular. The most
obvious it's probably on the way google serves it's results. Showing menu
fields parsed directly from HTML5, or visualizing data from their own internal
ontology XXX \url{https://en.wikipedia.org/wiki/Google_Knowledge_Graph}

What are the options for bringing sematic into a web? 

%In future they're going even beyond that and suggest "predictive search"
%\url{http://www.nytimes.com/2013/07/30/technology/apps-that-know-what-you-want-before-you-do.html}
%technologies, that try to guess the motivation of your search, not only it's
%meaning. \url{http://www.google.com/landing/now/}

One direction to go is to anotate data on "the server side", i.e. at the time
when we are creatign them. The person crating the data have to use the right
tool and spend time giving the data the appropriate annotation. There is enough
technologies for it: HTML5 adding tags for better annotation of the page
structure (such as nav, article, section, aside, ...), microformats
\url{http://microformats.org/} using html classes to bring standardized
patterns for several basic use cases with fixed structure, such as vcard or
event, or RDFa to annotate data on a webpage with an actual ontology (see in
separate section XXX).

There are tools for extracting and testing structured data 

\url{http://www.google.com/webmasters/tools/richsnippets}

\url{http://rdfa.info/play/}

To bypass the gap between anonymous data present on the web on one side and
rich, linked, meaningful ontologies on the other, we can go the opposite
direction as well. We can take the unannotated data already present on the web
and retrieve them in a form, that is defined by some ontology structure. 

To allow such a proccess we need to create tools that allow users to annotate
the, previously meaning-free, data with elements of existing ontology. By using
existing ontologies we not only give data the meaning, but also valuable
connection to any other dataset annotated using the same ontology. 



\sec Current solution crOWLer

The suggested base-technology is being developed on our faculty XXX. Crawler
called crOWLer serves the needs of extracting data from web. In current
technology, both, the scenario and the ontology structure/schema are hard-coded
into the crOWLer code. This requires unnecessary load of work for each separate
use case, whilst in practice all the use cases share the same workflow. 

\begitems \style n
  * load the ontology
  * add selectors to specific resources from the ontology
  * run the crawling process according the above
\enditems


\sec Proposed Solution and Methodology

To simplyfy the creation of guidelines, or scenarios for crOWLer, we propose a
tool that allows user to select all the element directly on the web page being
crawled, with all the necessary settings, pass the scenario created to the
crOWLer and obtain the results in a form of a graphical feedback. 

\sec Specific goals of the project

\begitems
  * implement extension for a browser
  * load and visualise ontology
  * create scenario for crOWLer 
  * serialize scenario and ontology
  * parse it by crOWLer creating it's configuration
  * run crOWLer
  * visualize the extracted data (feedback)
\enditems


\sec testy - co jsem zkousel..

TBD

\chap Existing solutions

%Semanticke a nesemanticke crowlery (linked media framework, calimachus, ...) je toho proste malo ;-) a proto to delame

\sec Semantic and non semantic crawlers

By researching existing solutions, there is currently no open source or openly
available solution to solve this task. Rumor goes there is proprietary tool in IBM.

Existing tools named as "Ontology-based Web Crawlers" refer mostly to crawlers
that "rank" pages being crawled by guess-matching them against some ontology.
In those programs user can't specify data that are being retrieved. Moreover,
there is no way to get involved in the crawling process. It is solely used to
automatically rank the relevance of documents. They are solving different task
where input is several documents and possibly an ontology and output is the
best matching document. 

In case we're solving  the input is one or more documents and one or more
ontologies and the result is data obtained from the documents and annotated
with structure from the ontologies. 

\sec Advantages and pitfalls of Semantic crawler and linked data

% vyhody sem crowleru, i pro linked data
The simplest approach is manual searching for keywords, or even simple browsing
the web. That might be useful in some cases, but when there is a lot of data,
it becomes exhausting. 

Crawling data using simple tools like 'wget --mirror' allows us to load data
and then write a program or script to retrieve a relevant information. This
approach takes a lot of energy for one time only solution of a given problem. 

By storing such crawled data into database we obtain persistent database,
possibly automatically obtained by the script from pervious case. Such data is
static, but can be queried over and over and possibly re-retrieved when becomes
obsolete. It's structure is, however, based on programmers imagination an
needst to be described in order to understand and handle the data properly. 

When using Ontology-based solution, tailor made for crawling and annotating
data from web, we obtain several benefits "for free". The tool designed
specially for this purpose makes it easy. Once the data is annotated, we can
not only query on them, but also automatically reason on them and obtain more
or more specific/narrow results than with general data. The atributes and
relations within ontology, that allow reasoning, are usually part of the
ontology deffinition and as such comes, again, "for free". 

Last for benefits: using ontology from public resource as a schema for our data
can give us correct structure without need for making it up or building it from
scratch. Also by using some common ontology, we can join together any
accessible data structured according to this ontology and simply query on
resulting super set. 

~

Semantic crawling is not a silver bullet. The technology is only finding it's
place and uses and it's being shaped by the needs of it's users. In current
it's mostly used on accademic field XXX. 

There is always a threat of inconsystency of an ontology when some data don't
fit the rules or breaks structure of an ontology. 

Just like with "hardcoded" crawling technique, the semantic crawling is tightly
connected with the structure of the web being crawled and selectors used for
matching data on the web. Any change on a webpage structure can lead to broken
selectors or links during the crawling process. 

A lot of web pages loads their data dynamically using AJAX queries. Some pages
simply changes it's content frequently (rt.com, vimeo.com, ...) which would
require almost constant crawling and growth into an massive ontology. 

Stating that, the semantic crawling is an usefull way to effectively obtain and
query otherwise anonymous data from the web, but it still have it's challenges
to overtake. 


\chap Knowledge base, principles and technologies

 Linked Data, RDFa, ...

informativni cast, teorie

Seznamte se technologiemi pro automatickou extrakci dat z webových stránek a s
jazyky sémantického webu RDF, RDFS a OWL.

\sec automatická extrakce dat

~

\sec RDF and RDFS

\begitems
 * \url{https://en.wikipedia.org/wiki/Resource_Description_Framework}
\enditems


\sec OWL

\begitems
 * \url{http://www.w3.org/TR/owl2-primer/}
 * \url{https://en.wikipedia.org/wiki/Web_Ontology_Language}
 * \url{http://www.w3.org/TR/2012/REC-owl2-quick-reference-20121211/}
\enditems


\sec Linked Data

\begitems
 * \url{http://linkeddata.org/guides-and-tutorials}
 * \url{http://linkeddatabook.com/editions/1.0/}
 * \url{http://lov.okfn.org/dataset/lov/}
\enditems


\sec Ontology repositories

\begitems
 * \url{http://www.w3.org/wiki/Ontology_repositories}
\enditems


\sec RDFa

\begitems
 * \url{https://www.sio2.cz/web/psiotwo/publications}
 * \url{http://rdfa.info/play/}
\enditems


\sec dalsi

\begitems
 * \url{https://en.wikipedia.org/wiki/SPARQL}
 * \url{https://en.wikipedia.org/wiki/Turtle_(syntax)}
\enditems






\chap Program design and Implementation


\sec Use Cases

\begitems
  * NPU
  * beerborec.cz
  * citybee.cz
\enditems



\sec Research - existující řešení - platforma


\secc InfoCram 2000 - Jirka Mašek

\begitems
  * zalozeny na Aardwark \urlnote{https://addons.mozilla.org/en-US/firefox/addon/aardvark/}
\enditems


\secc iMacros

\begitems
  * \url{http://wiki.imacros.net/Command_Reference}
  * \url{http://wiki.imacros.net/iMacros_for_Firefox}
  * \url{http://wiki.imacros.net/iMacros_for_Chrome}
\enditems


%\secc Sahi
%
%Yet another web automation and testing tool. \url{http://sourceforge.net/projects/sahi/}


\secc Selenium IDE

\begitems
  * IDE - \url{http://www.seleniumhq.org/projects/ide/}
  * plugins - \url{http://www.seleniumhq.org/projects/ide/plugins.jsp}
  * current commands - \url{http://release.seleniumhq.org/selenium-core/1.0.1/reference.html}
  * documentation - \url{http://docs.seleniumhq.org/docs/index.jsp}
  * extending selenium API (blog, tutorial) - \url{http://adam.goucher.ca/?s=selenium&paged=2}
  \begitems
    * randomString example - \url{http://adam.goucher.ca/?p=1348}
  \enditems
\enditems




\sec crOWLer

\secc zavislosti

\begitems \style O
  * maven - apache project managing tool
  \begitems \style o
    * \url{https://maven.apache.org}
    * \url{https://maven.apache.org/run-maven/index.html}
    * \url{https://maven.apache.org/guides/mini/guide-ide-eclipse.html}
  \enditems
  * sesame
  \begitems \style o
    * \url{http://www.openrdf.org/download.jsp} ??
  \enditems
  * jena
  \begitems \style o
    * \url{https://github.com/ansell/JenaSesame} !!
    * or \url{https://github.com/afs/JenaSesame} ??
    * or \url{http://jena.apache.org/} ???
    * or \url{http://sjadapter.sourceforge.net/} ????
    * or \url{http://sourceforge.net/projects/jenasesamemodel/}
    * might help \url{http://www.iandickinson.me.uk/articles/jena-eclipse-helloworld/}
    * little hint \url{http://spqr.cerch.kcl.ac.uk/?page_id=130}
    * another hit \url{http://answers.semanticweb.com/questions/20865/how-to-get-the-jena-sesame-adapter}
    * wiki \url{https://en.wikipedia.org/wiki/Jena_(framework)}
    * jena vs. sesame flame \url{http://answers.semanticweb.com/questions/1638/jena-vs-sesame-is-there-a-serious-complete-up-to-date-unbiased-well-informed-side-by-side-comparison-between-the-two}
  \enditems
\enditems


\secc Classes of CrOWLer

\begitems
  * ImmovableHeritageConfiguration extends MonumnetConfiguration implements ConfigurationFactory 
  \begitems
    * implements Configuration, which is parameter for FullCrawler.run() method
  \enditems
  * FullCrawler
  \begitems
    * implements the whole crawling algorithm
    * 
  \enditems
\enditems

\secc Run configuration 

\begtt
crowler cz.sio2.crowler.configurations.npu.ImmovableHeritageConfiguration file results
crowler cz.sio2.crowler.configurations.kub1x.KbxConfiguration file results
crowler cz.sio2.crowler.configurations.parser.SeleniumConfiguration\
         file results generated.html
\endtt

\begitems
  * Class ImmovableHeritageConfiguration implements Configuration class. 
  * Folder jena\_con will be created and all the rdf's will be stored in int with names derived from ontology uri
\enditems




\sec Model




\sec Imlementation




\sec Issues - solved and unsolved

\begitems
  * error handling (non existent selector, missing data, ...)
\enditems

% * diagramy... clas, sequence (komunikace s jOWLem, komunikace s generatorem selectoru, ), 
% * model
% * vyresene, nevyresene problemy













\chap Results and Tests 
% * na npu, nebo i na jinych strankach


\sec Data


\secc Pamatky

\begitems
  * \url{http://onto.mondis.cz/resource/page/npu/}
  * \url{http://monumnet.npu.cz/pamfond/list.php?hledani=1&KrOk=&HiZe=&VybUzemi=1&sNazSidOb=&Adresa=&Cdom=&Pamatka=&CiRejst=&Uz=B&PrirUbytOd=3.5.1958&PrirUbytDo=10.12.2013}
  * \url{http://dominanty.cz/pamatky-cihana.php}
\enditems





\chap zaver

TBD


\bye
