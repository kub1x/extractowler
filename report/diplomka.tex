\input ctustyle
\worktype [M/EN]
\faculty {F3}
\department {Department of Computer Science and Engineering}
\title {Platform for semantic extraction of the web}
\titleEN {Platform for semantic extraction of the web}
\titleCZ {Platforma pro sémantickou extrakci webu}
\author {Jakub Podlaha}
%\authorinfo {podlajak@fel.cvut.cz}
%\studyinfo {Artificial Intelligence}
%\supervisor {Ing. Petr Křemen, Ph.D.}
\date {January 2015}
\abstractEN {This document is for testing purpose only.}
\abstractCZ {Tento dokument je pouze pro potřeby testování.}
\declaration {Prohlašuji, že jsem se neflákal.}
\makefront



\chap Introduction

During past few years the Web went through bigger or smaller revolutions. 

\begitems
  * WEB 2.0 and tag cloud
  * HTML5 and semantic tags
  * Smartphones, Tablets and mobile web everywhere, 
  * The run out of IPv4 addresses, nonexistent boom of IPv6, 
  * Cloud technologies and BigData, 
  * Bitcoin, Tor, anonymous internet, 
  * WikiLeaks, NSA, Heartbleed and secourity concerns
  * Google Knowledge Graph, Facebook Open Graph, ...
\enditems

That's only few examples of some of the biggest recent issues on the web in
general. We live in an age, where so little can mean so much. The enviroment
online is dramatically changing, mostly on a wave of some new, useful or
frightening technology. The Semantic Web technologies have been described,
standardized and implemented for several years now  (XXX Example with linked
data, rdf) and their tide seems to be near, though yet to come.

Semantic Web itself reates to several principles (along with their
implementation) that allow users to add meaning to their data. This meaning
brings not only a standardized structure, but also, as a consequence, the
posibility to query and reason on data from multiple sources. Once given the
structure, similar data can be joined in a form of a bigger cloud. This
phenomena is called Linked Data. 

In this work we'd like to bring the Semantic Web technologies closer to users.
The approach is to propose a methodology for extracting structured data out of
unstructured ones, designing and implementing an appropriate tool, to simplify
the process of annotating (yet anonymous) data on a webpage, i.e. to bring
structure and meaning into it. 



\sec Problem Statement and Motivation

Giving meaning, i.e. semantization of web pages gets more popular. Probably the most
obvious example can be seen in the way the Google search engine serves it's
results. Presenting not only the resulting pages but as well snippets of
information scraped directly from the page content such as menu fields parsed
directly from HTML5, contact information or openning hours, or even visualizing
data from their own internal ontology, the Knowledge Graph. 

XXX \url{https://en.wikipedia.org/wiki/Google_Knowledge_Graph}

XXX Strigil - \url{http://delivery.acm.org/10.1145/2540000/2539170/p453-starka.pdf}

What are the options for bringing sematic into a web? 

%In future they're going even beyond that and suggest "predictive search"
%\url{http://www.nytimes.com/2013/07/30/technology/apps-that-know-what-you-want-before-you-do.html}
%technologies, that try to guess the motivation of your search, not only it's
%meaning. \url{http://www.google.com/landing/now/}

One direction to go (XXX better) is to annotate data on "the server side", i.e. at the time
it is being created and/or published. The person or engine creating the data have to use
the right tool and spend time giving the data the appropriate annotation. There
are standards covering this use case: HTML5 brings in tags for clearer
specification of the page structure (such as nav, article, section, aside,
...). Microformats \url{http://microformats.org/} define specialized values for
HTML class atribute to bring standardized patterns for several basic use cases
with fixed structure, such as vCard or Event. The microformat approach is easy
to implement as it doesn't impose any extra technology into the stuck(XXX
better). Last but not least, we can use RDFa to annotate data on a webpage with an
actual ontology. This technology is part of the Semantic Web stack and we'll
describe it closer in further chapter (see in separate section XXX).

Annotating data on the server side enables users to use tools highlight data
they are specifically interested in, extract them and reason on them. Services
can use annotated data, combine them and offer results from multiple sources. 

Some examples of utilities for extracting and testing structured data: 

\url{http://www.google.com/webmasters/tools/richsnippets}

\url{http://rdfa.info/play/}

To bypass the gap between anonymous data present on the web on one side and
rich, linked, meaningful ontologies (XXX example) on the other, we can go the
opposite direction as well. We can take the unannotated data already present on
the web and retrieve them in a form, that is defined by some ontology
structure. This process can be performet automatically or manually. 

There've been several (XXX) attempts in the automatic data annotation. In
principle, the goal of this approach is to automatically analyze a web page,
find the most appropriate ontology for it and use this ontology to
automatically annotate data on the page. 

As an example, on a webpage of a music band this would find (XXX
http://www.musicontology.com/) as the best matching schema, and it would
annotate band name, genre, albums using classes and  properties from this ontology. 

While on well structured, simple and/or partially annotated pages this process
can be very successfull and produce usefull results, on pages with unorganized
data the confidence on results produced by this approach might drop to loterry (XXX). 
Unfortunately many online webpages and services are poorly strucruted. Pages
containing many unrealted data, in form of advertisements or other external
content might confuse such an engine. Old servers present their content in
poorly structured or even invalid HTML in a form of multiple nested tables that
serve for structuring only and makes the whole structure of the web unreadable
(XXX). Social aspect of web brings in almost complete randomness making it
even harder to automatically reason on page's content if it can't be
distinguished and potentially left aside. To sum it up, there are many
potential and real threads that prevent us from automatically annotate all data
on web with confidence (XXX). 

In some use cases the ontology of the desired data is yet to be created and the
user is aware of the data strucrutre and capable of manually spot and select
the data on a web page. Currently there isn't many tools allowing this kind of
operation. The ideal implementation and the vision here will allow user to
partially identify the structure fo a webpage while leaving the repeatative
tedious work on crawler following the same structure on all data on webpage. 

To allow such a proccess we need to create tools that allow users to annotate
the, previously meaning-free, data with elements of existing ontology and/or
create resources on-the-go. By using existing ontologies we not only give the
meaning to our data, but also valuable connection to any other dataset
annotated using the same ontology. 


\sec Current solution crOWLer

The suggested base-technology is being developed on our faculty XXX. Crawler
called crOWLer serves the needs of extracting data from web. In current
implementation, both, the scenario, followed by the crawler, and the ontology
structure/schema are hard-coded into the crOWLer code. This requires
unnecessary load of work for each separate use case, whilst in practice all the
use cases share the same workflow. 

\begitems \style n
  * load the ontology
  * add selectors to specific resources from the ontology
  * implement the rules to follow another page
  * run the crawling process according the above
\enditems



\sec Proposed Solution and Methodology

To simplyfy the creation of guidelines, or scenarios for crOWLer, we propose a
tool that allows user to select all the element directly on the web page being
crawled, with all the necessary settings, pass the scenario created to the
crOWLer and obtain the results in a form of a graphical feedback. 

\sec Specific goals of the project

\begitems
  * design the semantic data creation use-cases
  * implement extension for a browser
  * load and visualise ontology
  * create scenario for crOWLer 
  * serialize scenario and ontology
  * parse it by crOWLer creating it's configuration
  * run crOWLer
  * visualize the extracted data (feedback)
\enditems


\sec Work structure (XXX)

TBD











\chap Knowledge base, principles and technologies

\sec Technology of Semantic Web

Wikipedia defines Semantic Web as a collaborative movement led by international
standards body the World Wide Web Consortium (W3C).  (XXX
\url{https://en.wikipedia.org/wiki/Semantic_Web}) W3C itself defines Semantic
Web as a technology stack to support a "Web of data," as opposed to "Web of
documents," the web we commonly know and use (XXX
\url{http://www.w3.org/standards/semanticweb/}). Just like with "Cloud" or "Big
Data" the propper definition tends to vary, but the notion remains the same.
It is collaborative movement led by W3C and it does define a technology stack.
It also includes users and companies using this technology and the data itself. 
Technologies and languages of Semantic Web such as RDF, RDFa, OWL, SPARQL (XXX)
are well standardized and will be described in following chapters. 

As a general logical concept of the technology... (XXX) Technology of Semantic
Web is used to take data and metadata, give them unique identifiers and form
them into oriented graphs. The metadata part define a schema of types and
properties that can be assigned to data and also relations between this types
and properties possibly in a form of ontology. When some data are anotaded by
resources from such an ontology we gain power to reason on this data, i.e.
resolve new relations based on known ones, and also to query on our data along
with any data annotated using the same ontology. 

In RDF(S) the  is defined in a form of triples. Triple consists of subject,
predicate and object, wich all are simply "resources" listed by their
identifiers. In this very general form we can express basically any
relationship between two resources. On a level of classes and properties, we
can for example assign a type to an individual, or set a class as a domain of
some property. On a level of ontologies we can specify author and date it was
released. (XXX DELME) 

\sec RDF and RDFS

RDF is a family of specifications for syntax notations and data serialization
formats, meta data modeling, and vocabulary used for it. 

XXX \url{https://en.wikipedia.org/wiki/Resource_Description_Framework}

We will look closely on URI, the resource identifier, vocabularies and
semantics defined by RDF, RDFS, and OWL, and serialization into Turtle and
RDF/XML formats. 

\secc URI

In order to give each resource an unique identifier a Uniform Resource
Identifier is used. This is mostly in a form of URL as we commonly know it as
"web address" (e.g. http://www.example.org/some/place\#something). In some cases
URI can be a URN as well. URN is a complementary syntax for URL that allow us
to identify resources without specifying their location. This way we can for
example use ISBN codes when working with books and records, or UUID identifier
a Universally Uniqe Identifier widely used to identify technically any data
instance. 

XXX \url{https://en.wikipedia.org/wiki/Uniform_resource_identifier}

\secc RDF and RDFS vocabulary

In order to work with data properly (XXX) RDF(S) vocabulary defines several basic URIs along with their semantics. 

    rdf:type is a property used to state that a resource is an instance of a class. A commonly accepted qname for this property is "a".[4]

    rdfs:Resource - is the class of everything. All things described by RDF are resources.
    rdfs:Class    - declares a resource as a class for other resources.

    rdfs:Literal  – literal values such as strings and integers. Property values such as textual strings are examples of RDF literals. Literals may be plain or typed.
    rdfs:Datatype – the class of datatypes. rdfs:Datatype is both an instance of and a subclass of rdfs:Class. Each instance of rdfs:Datatype is a subclass of rdfs:Literal.
    rdf:XMLLiteral – the class of XML literal values. rdf:XMLLiteral is an instance of rdfs:Datatype (and thus a subclass of rdfs:Literal).

    rdf:Property – the class of properties.
    rdfs:domain of an rdf:predicate declares the class of the subject in a triple whose second component is the predicate.
    rdfs:range of an rdf:predicate declares the class or datatype of the object in a triple whose second component is the predicate.

    rdfs:subClassOf allows to declare hierarchies of classes.
    rdfs:subPropertyOf is an instance of rdf:Property that is used to state that all resources related by one property are also related by another.

    rdfs:label is an instance of rdf:Property that may be used to provide a human-readable version of a resource's name.
    rdfs:comment is an instance of rdf:Property that may be used to provide a human-readable description of a resource.

These are the basic building blocks of our future RDF graphs. The semantics
defined in the specification and slightly described here alow us to specify
class hierarchy, properties with domain and range as well as use this structure
on individuals and literals.  

\sec OWL

\begitems
 * \url{http://www.w3.org/TR/owl2-primer/}
 * \url{https://en.wikipedia.org/wiki/Web_Ontology_Language}
 * \url{http://www.w3.org/TR/2012/REC-owl2-quick-reference-20121211/}
\enditems


\sec Linked Data

Wikipedia defines Linked Data as "a term used to describe a recommended best
practice for exposing, sharing, and connecting pieces of data, information, and
knowledge on the Semantic Web using URIs and RDF."

\begitems
 * \url{http://linkeddata.org/guides-and-tutorials}
 * \url{http://linkeddatabook.com/editions/1.0/}
 * \url{http://lov.okfn.org/dataset/lov/}
\enditems


\sec Ontology repositories

\begitems
 * \url{http://www.w3.org/wiki/Ontology_repositories}
\enditems


\sec RDFa

\begitems
 * \url{https://www.sio2.cz/web/psiotwo/publications}
 * \url{http://rdfa.info/play/}
\enditems


\sec dalsi

\begitems
 * \url{https://en.wikipedia.org/wiki/SPARQL}
 * \url{https://en.wikipedia.org/wiki/Turtle_(syntax)}
\enditems



\sec automatická extrakce dat

TODO in next section














\chap Existing solutions

%Semanticke a nesemanticke crowlery (linked media framework, calimachus, ...) je toho proste malo ;-) a proto to delame

\sec Semantic and non semantic crawlers

By researching existing solutions, there is currently no open source or openly
available solution to solve this task. Rumor goes there is proprietary tool in IBM.

Existing tools named as "Ontology-based Web Crawlers" refer mostly to crawlers
that "rank" pages being crawled by guess-matching them against some ontology.
In those programs user can't specify data that are being retrieved. Moreover,
there is no way to get involved in the crawling process. It is solely used to
automatically rank the relevance of documents. They are solving different task
where input is several documents and possibly an ontology and output is the
best matching document. 

In case we are trying to solve the input is one or more documents and one or
more ontologies and the result is data obtained from the documents and
annotated with resources from the ontologies. 

\sec Advantages and pitfalls of Semantic crawler and linked data

% vyhody sem crowleru, i pro linked data
The simplest approach is manual searching for keywords, or even simple browsing
the web. That might be useful in some cases, but when there is a lot of data,
it becomes exhausting. 

Crawling data using simple tools like 'wget --mirror' allows us to load data
and then write a program or script to retrieve a relevant information. This
approach takes a lot of energy for one time only solution of a given problem. 

By storing such crawled data into database we obtain persistent database,
possibly automatically obtained by the script from pervious case. Such data is
static, but can be queried over and over and possibly re-retrieved when becomes
obsolete. It's structure is, however, based on programmers imagination an
needs to be described in order to understand and handle the data properly. 

When using Ontology-based solution, tailor made for crawling and annotating
data from web, we obtain several benefits "for free". The tool designed
specially for this purpose makes it easy. Once the data is annotated, we can
not only query on them, but also automatically reason on them and obtain more
or more specific/narrow results than with general data. The atributes and
relations within ontology, that allow reasoning, are usually part of the
ontology deffinition and as such comes, again, "for free". 

Last for benefits: using ontology from public resource as a schema for our data
can give us correct structure without need for XXX making it up or building it
from scratch. Also by using some common ontology, we can join together any
accessible data structured according to this ontology and simply query on
resulting super set. With this approach we can utilize the power of linked data
cloud (XXX reference). 

~

Semantic crawling is not a silver bullet. The technology is only finding it's
place and uses and it's being shaped by the needs of it's users. In current
it's mostly used on accademic field XXX. 

There is always a threat of inconsistency of an ontology when some data don't
fit the rules or breaks structure of an ontology. (XXX more)

Just like with "hardcoded" crawling technique, the semantic crawling is tightly
connected (XXX better) with the structure of the web being crawled and
selectors (XXX explain term) used for matching data on the web. Any change on a
webpage structure can lead to broken selectors or links during the crawling
process (XXX and make the scenario useles, more on self-repairing of
scenarios?). 

A lot of web pages loads their data dynamically using AJAX queries. Some pages
simply changes it's content frequently (XXX typically news pages, forums: rt.com,
vimeo.com, ...) which would require almost constant crawling and growth into an
massive ontology (XXX any suggestions on that? =). 

Stating that, the semantic crawling is an usefull way to effectively obtain and
query on (otherwise anonymous) data from the web, but it still have it's challenges
to overtake. 




\sec Research - existující řešení - platforma


\secc InfoCram 2000 - Jirka Mašek

\begitems
  * zalozeny na Aardwark \urlnote{https://addons.mozilla.org/en-US/firefox/addon/aardvark/}
\enditems


\secc iMacros

\begitems
  * \url{http://wiki.imacros.net/Command_Reference}
  * \url{http://wiki.imacros.net/iMacros_for_Firefox}
  * \url{http://wiki.imacros.net/iMacros_for_Chrome}
\enditems



%\secc Sahi
%
%Yet another web automation and testing tool. \url{http://sourceforge.net/projects/sahi/}



\secc Selenium IDE

\begitems
  * IDE - \url{http://www.seleniumhq.org/projects/ide/}
  * plugins - \url{http://www.seleniumhq.org/projects/ide/plugins.jsp}
  * current commands - \url{http://release.seleniumhq.org/selenium-core/1.0.1/reference.html}
  * documentation - \url{http://docs.seleniumhq.org/docs/index.jsp}
  * extending selenium API (blog, tutorial) - \url{http://adam.goucher.ca/?s=selenium&paged=2}
  \begitems
    * randomString example - \url{http://adam.goucher.ca/?p=1348}
  \enditems
\enditems






\sec crOWLer

\secc zavislosti

\begitems \style O
  * maven - apache project managing tool
  \begitems \style o
    * \url{https://maven.apache.org}
    * \url{https://maven.apache.org/run-maven/index.html}
    * \url{https://maven.apache.org/guides/mini/guide-ide-eclipse.html}
  \enditems
  * sesame
  \begitems \style o
    * \url{http://www.openrdf.org/download.jsp} ??
  \enditems
  * jena
  \begitems \style o
    * \url{https://github.com/ansell/JenaSesame} !!
    * or \url{https://github.com/afs/JenaSesame} ??
    * or \url{http://jena.apache.org/} ???
    * or \url{http://sjadapter.sourceforge.net/} ????
    * or \url{http://sourceforge.net/projects/jenasesamemodel/}
    * might help \url{http://www.iandickinson.me.uk/articles/jena-eclipse-helloworld/}
    * little hint \url{http://spqr.cerch.kcl.ac.uk/?page_id=130}
    * another hit \url{http://answers.semanticweb.com/questions/20865/how-to-get-the-jena-sesame-adapter}
    * wiki \url{https://en.wikipedia.org/wiki/Jena_(framework)}
    * jena vs. sesame flame \url{http://answers.semanticweb.com/questions/1638/jena-vs-sesame-is-there-a-serious-complete-up-to-date-unbiased-well-informed-side-by-side-comparison-between-the-two}
  \enditems
\enditems


\secc Classes of CrOWLer

\begitems
  * ImmovableHeritageConfiguration extends MonumnetConfiguration implements ConfigurationFactory 
  \begitems
    * implements Configuration, which is parameter for FullCrawler.run() method
  \enditems
  * FullCrawler
  \begitems
    * implements the whole crawling algorithm
    * 
  \enditems
\enditems

\secc Run configuration 

\begtt
crowler cz.sio2.crowler.configurations.npu.ImmovableHeritageConfiguration file results
crowler cz.sio2.crowler.configurations.kub1x.KbxConfiguration file results
crowler cz.sio2.crowler.configurations.parser.SeleniumConfiguration\
         file results generated.html
\endtt

\begitems
  * Class ImmovableHeritageConfiguration implements Configuration class. 
  * Folder jena\_con will be created and all the rdf's will be stored in int with names derived from ontology uri
\enditems



\sec Strigil!

\secc What problems it solves? (Use cases)

\secc Architecture of Strigil platform

\secc What inspiration it brings for crowler






\chap Program design


\sec Use Cases

\begitems
  * NPU
  * RLP
  * beerborec.cz
  * citybee.cz
\enditems



\sec Workflow

\secc Main line

\begitems
  * user loads/creates ontology using sowl
  * user opens webpage with data
  * user creates scenario using sowl
  * sowl sends scenario to crowler
  * crowler crawls the web according to scenario and stores results in repository
  * ~ + crowler sends data to sowl which embeds them in original web page (XXX)
\enditems


\secc Scenario creation

\begitems
  * user starts scenario creation in sowl
  * loop until finished:
  \begitems
    * user selects an element on page
    * user select action on element (perform and record event, i.e. click on link, narrow HTML context, assign element - object or property according to situation, ...)
    * sowl records the action in scenario
  \enditems
\enditems


\secc Additional branches to Scenario Creation

\begitems
  * user can navigate through scenario by clicking scenario steps
  * user can navigate through scenario by clicking ontological context
  * user can navigate through scenario by clicking areas on webpage covered by scenario
  * when user clicks on a hyperlink: 
  \begitems
    * existing template can be assigned to the action (no need to actually follow the link)
    * new tamplete can be created for resulting action (resulting page loaded, new template created, click through shown in breadcrumbs)
  \enditems
\enditems



\sec Model

~


\sec Imlementation

~


\sec Issues - solved and unsolved

\begitems
  * error handling (non existent selector, missing data, ...)
\enditems

% * diagramy... clas, sequence (komunikace s jOWLem, komunikace s generatorem selectoru, ), 
% * model
% * vyresene, nevyresene problemy




\chap Program Implementation
~







\chap Results and Tests 
% * na npu, nebo i na jinych strankach


\sec Data
~

\secc Pamatky

\begitems
  * \url{http://onto.mondis.cz/resource/page/npu/}
  * \url{http://monumnet.npu.cz/pamfond/list.php?hledani=1&KrOk=&HiZe=&VybUzemi=1&sNazSidOb=&Adresa=&Cdom=&Pamatka=&CiRejst=&Uz=B&PrirUbytOd=3.5.1958&PrirUbytDo=10.12.2013}
  * \url{http://dominanty.cz/pamatky-cihana.php}
\enditems





\chap zaver

TBD


\bye
