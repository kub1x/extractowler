\label[chap:design]
\chap Program stack design

This chapter defines the overall behavior of the program stack derived from
presented use cases.

TODO use cases analysis here?  XXX XXX XXX 

\sec Workflow

From the use cases defined and from analysis performed on existing solution we
can derive the general workflow for both SOWL and crOWLer part of the
implementation. 

\midinsert
%\clabel[general_workflow]{An activity diagram of the general workflow of the stack}
\picw=14cm \cinspic general-workflow.png
%\caption/f Diagram of general workflow as derived from presented use case
\endinsert

~

\secc Main line

\begitems
  * user loads/creates ontology using SOWL
  * user opens webpage with data
  * user creates scenario using SOWL
  \begitems
    * user adds selectors to scenario steps
    * user adds resources to scenario steps
  \enditems
  * SOWL sends scenario to crOWLer
  * crOWLer crawls the web according to scenario and stores results in a file
    or repository
\enditems


\secc Scenario creation

\begitems
  * user starts scenario creation in SOWL
  * loop until finished:
  \begitems
    * user creates a step in scenario
    * user selects an element on page, a selector is generated if applicable,
      on the step
    * user selects a resource, resource is updated on the appropriate field of
      the step, if applicable
  \enditems
\enditems


\secc Additional branches to Scenario Creation

\begitems
  * user can navigate through scenario by clicking scenario steps
  * user can navigate through scenario by clicking ontological context
  * user can navigate through scenario by clicking areas on webpage covered by scenario
  * when user clicks on a hyperlink: 
  \begitems
    * existing template can be assigned to the action (no need to actually
      follow the link)
    * new template can be created for resulting action (resulting page loaded,
      new template created)
  \enditems
\enditems

\secc crOWLer scraping

\begitems
  * user runs crOWLer passing it the created scenario
  * crOWLer parses the scenario
  * crOWLer scrapes data from the webpage following the scenario
  * crOWLer stores the results in file or repository
\enditems




\sec Designing scenario format

One of main tasks of this work was to create format for scenario generated by
SOWL and consumed by crOWLer. This scenario will describe information necessary
for the crawling process: what operation to do (create ontological object,
assign property to such an object, perform task with webpage). 

This task is closely related to implementation peculiarity of semantic crawler:
we are dealing with two separate contexts at the same time, the ontological and
the web context. Ontological context holds current object (individual) to which
we assign properties, web context hold current webpage along with currently
selected element on that webpage. Scenario have to support operations to change
each context separately and/or both at the same time. 



\secc Strigil/XML

Strigil, the scraping platform in order to solve similar problem as crOWLer
introduces its own XML based Scraping Script format. Its documentation can be
found here XXX
\urlnote{https://drive.google.com/file/d/0B4On-lGb38CgWlAyZDhGbDV2TFk/edit
Scraping script documentation}. 

Basis of the whole script is system of {\em templates}. Each template has a name
and mime-type declaring type of document the template is designed for. This
information is needed as Strigil supports HTML and also Excel spreadsheet
files. Templates call each other using {\tt call-template} command anywhere in
the script. This command accepts URL as an argument from its nested commands.
Each template is called only with new URL, thus on new document. Of course URL
of current document can be passed as an argument, but due to nature of Strigil,
this would create completely separate context. 

Strigil is tailor made for parallel processing. The architecture of the Strigil
system contains not only scraping processor, but also a layer for distributed
download queue processing and layer of proxy servers that can be used to spread
the traffic and scale the download process horizontally. As the downloads are
performed asynchronously and can be even delayed due to network lags and
timeouts, there is no guaranteed order in which documents will be scraped.
Each of Strigil templates create its own context when called. If we want to
link data obtained from different template calls we have to use some additional
techniques. For example we can assign some properly defined, non-random, unique
identifiers to an object. This identifier have to be guaranteed to be the same
for the same object through different template calls and potentially on
different pages. 

To handle ontological data manipulation the commands {\tt onto-elem} and {\tt
value-of} are used. First one creates an individual of given type and, if
nested into different {\tt onto-elem} relates this new individual to its
parent with some property. Literals are assigned to properties of parent object
using {\tt value-of} command with property name specified. This command is very
powerful with usage regular expressions, selectors or nested calls of itself it
can create arbitrary values from constants and data obtained from web page
being processed. 

Strigil also implements variety of functoins to help with processing of textual
data. Function {\tt addLanguageInfo}, for example, is widely used in Strigil
scraping scripts to add language tags to string literals. The function call can
be seen below. 

\begtt
<scr:function name="addLanguageInfo">
  <scr:with-param>
    <scr:value-of select="Hello World" />
  </scr:with-param>
  <scr:with-param>
    <scr:value-of text="en" />
  </scr:with-param>
</scr:function>
\endtt

Similarly we can use function {\tt addDataTypeInfo} to add datatype flag,
function {\tt generateUUID} to obtain unique identifier or function {\tt
convertDate} to convert Czech and English dates into a common {\tt xsd:date}
format and several others. Some functions, like the last one mentioned, cover
task-specific issues and Strigil does not define a way to extend the list of
functions. 

In early stages of SOWL development an attempt was made to use original
Strigil/XML as a format of choice. An appropriate, consistent subset was chosen
that would cover required use cases. Implementation of simple use cases
revealed some pitfalls of this decision and revealed several suggestions for
improvements on the approach and the format itself. 


\secc Adaptation of Strigil/XML format

Strigil creates its scraping script internally hidden under GUI and leaves
user unaware of its actual content. It might still serve well, at least for
developers, to keep the script compact and easily readable. Addition of
language tag as seen in previous chapter, is widely used pattern that polutes
the resulting script with unnecessary overload. Suggested improvement would
separate this functionality into an extra attribute of the {\tt value-of} tag
named {\tt lang}.

The same suggestion can be applied to the data-type specification. Moreover
{\em implicit parsing} of known data types would not only simplify the scraping
script, but also help to clean and clear the resulting data. 

Let us imagine hypothetical scenario of two similar tables on one page
containing two sets of data in the same format. For such a case we would need
to define a template on subset of DOM and call it twice with different root
node. Creation of {\tt dom-template} and {\tt call-dom-template} tags would
solve this issue and would allow scenario creator to narrow down his focus to a
subpart of the scraped webpage. This would be particularly useful on
complicated pages with a lot of nested HTML. {\tt dom-template} and {\tt
call-dom-template} would be defined within a single {\tt template} tag and
unlike {\tt call-template}, they would {\em keep} the ontological context co
call of {\tt value-of} within {\tt dom-template} would assign a property to
individual created by {\tt onto-elem} wrapping the current {\tt
call-dom-template} call. 

The architecture of Strigil (distributed downloader) suggests that it uses
simple raw HTML pages as they were downloaded and uses JSOUP to extract data
from it as JSOUP is the selector system of choice. Many webpages, or even web
applications, make use of dynamic AJAX calls to fetch additional data after the
presentation layer of the web is shown to the user. Strigil does not handle
these cases by default. The internal AJAX code could be analyzed and simulated
using {\tt call-template} call, but this requires deep knowledge of the webpage
being processed. In crOWLer we opted to switch from JSOUP to WebDriver library
and use PhantomJs, a no-GUI web browser. This technology allow us to handle
webpages the same way as user sees them. 

Usage of actual full-stack web browser with JavaScript engine long with
WebDriver allows us to inject and execute arbitrary JavaScript code into the
processed webpage. In order to make full use of this feature we can define {\tt
function-def} tag which would define JavaScript function with name and
parameters and contain its code. To execute this function we would call {\tt
function-call} and identify it by its name. Return value of this function can
be then used the same way as the one from {\tt value-of} tag. 

From the experience with development on Strigil/XML we can derive, that it is
tied with its intended use for distributed downloader and it lacks some
functionality. In SOWL we would almost necessarily modify its formal definition
and thus it is of consideration if we can not make use of more appropriate
format. 

% Strigil wiki on sf \url{http://sourceforge.net/p/strigil/home/Home}

\secc SOWL/JSON

As all Firefox extensions, SOWL is written entirely using JavaScript with
additional HTML defining the graphical layout. Early stages of implementation
generated XML based on Strigi/XML format using hardcoded XML snippets and
string formating -- approach often used on webpages with dynamically loaded
content. A string holds a snippet of HTML or XML structure with placeholder.
This placeholder is replaced by either a value or by another already processed
snippet. This way piece by piece the whole scenario is generated. This solution
is not hard to implement, but brings in poor maintainability and with additional
complexity it looses elegance, readability and can even cause performance
issues. 

Original data of the scenario created by SOWL are stored naturally in
JavaScript object. Using standard JavaScript method {\tt JSON.stringify()} we
can immediately generate JSON serialization of such object. This way we have
structure similar to the original defined by Strigil/XML, but in flexible
structure. Obviously some adaptations are necessary. Nesting is recorded using
the {\tt steps}, the header section is redesigned for the JSON structure. For
example instead of listing prefixes in a single string of XML attribute, we
define object ontology with a map of prefix--URI pairs. 

The original semantics of {\tt onto-elem} and {\tt value-of} was preserved,
only limited to its basic use. {\tt value-of} serves to assign literal
properties or to retrieve textual values for its parent scenario step. 

An example of the scraping script can be found in appendix~\ref[app:sowljson]. 


\secc Consequences of conversion to JSON format

According to difference in syntax between XML and JSON do not have "text
content" of elements like XML elements can. In JSON we simply reserve a
property for a value that would be otherwise specified this way in
corresponding XML. Strigil, however, does not explicitly use the textual values
and everything is specified using attributes. Some elements return textual
values to their parents to handle, and in these cases it might be suitable to
enable textual values as constants instead of the required element. 

Another syntactical distinction is that JSON does not explicitly define child
nodes. Everything is property in JSON object, so we, again, assign a property
to store the child nodes. Child nodes are held in ordered list which in
JavaScript corresponds to an array. As we build a structure of scenario steps,
the reserved property will be simply called {\tt steps} for every element that
allows child nodes (e.g. {\tt onto-elem} or {\tt template}). 

Technically each JSON object quacks like a hash
map~\urlnote{https://en.wikipedia.org/wiki/Duck_typing} with a string keys and
value of any JavaScript type. We can benefit from this loose structure. For
example we can use any key to store a substep, not only the {\tt steps} array. 

The {\tt onto-elem} command benefits exactly from this difference between
XML and JSON. In original Strigil/XML the {\tt onto-elem} tag allow us to
specify URI of the resulting individual (as commonly denoted by the {\em about}
property), by taking it from from its {\em first child} which is expected to
be {\tt value-of} tag. Needles to say, this specification lowers robustness as
the position in the XML file is not enforced by the syntax and can be easily
unintentionally broken by accidental swap of two elements, although it would
not invalidate the files syntax and thus would not be captured by the script
parser as an error. In the JSON format we lack the notion of child elements.
Even when we simulate it as mentioned before, we would only cause the same
indetermination. So instead, we simply reserve a property named {\tt about}
exactly for the described use.


\label[sec:javascript]
\sec JavaScript and events support

Special attention have to be payed when dealing with direct interaction with
DOM elements and script execution. WebDriver supports injection and execution
of JavaScript as well as simulation of user interactions like {\em click} on
element or {\em back} and {\em forward} navigation. Even though it brings great
power there are considerations and great limitations to be taken in account. 

WebDriver supports execution of JavaScript directly on webpage loaded in the
driver. This is done by calling {\tt executeScript} or {\tt executeAsyncScript}
function on the {\tt driver} object. First argument of these functions is
string defining content of JavaScript function we want to execute. Header and
actual call of this function will be added for us before it gets attached to
the webpage. We can pass any number of accepted arguments to these functions
and they will be accessible through standard {\tt arguments} object in on the
JavaScript side. Types, corresponding to standard JavaScript types are
supported as arguments: number, boolean, String, WebElement or List of any
combination of the previous~\urlnote{http://goo.gl/Hhwq3l Selenium
JavaScriptExecutor documentation}. The second -- asynchronous version returns
immediately with a {\tt response} object. It provides callback as additional
argument to the JavaScript call. This callback is used for synchronization when
accessing the result on the {\tt response} object from Java.  

%XXX TODO better example this one is from
%here~\urlnote{http://docs.seleniumhq.org/docs/03_webdriver.jsp\#using-JavaScript}

\begtt
JavaScriptExecutor exec = (JavaScriptExecutor)driver;
List<WebElement> labels = driver.findElements(By.tagName("label"));
List<WebElement> inputs = (List<WebElement>) exec.executeScript(
    "var labels = arguments[0]," +
    "    inputs = [];" +
    "for (var i = 0; i < labels.length; i++) {" +
    "  var name = labels[i].getAttribute('for');" +
    "  inputs.push(document.getElementById(name));" +
    "} return inputs;", labels);
\endtt

In simple cases we can use JavaScript to extend functionality of crOWLer. It
might be used as a complex string formatter, parser for nontrivial values etc. 
In following example it is used to condition on attribute value of an anchor
tag. A document location if the href tag contains a hash symbol "#" (often 
used when the link is handled by JavaScript function. 

\begtt
JavaScriptExecutor exec = (JavaScriptExecutor)driver;
WebElement el = driver.findElement(By.cssSelector('a.detail'));
String result = (String) exec.executeScript(
"var elem = arguments[0];"+
"var href = elem.getAttribute('href');" +
"return (href ==='#' ? window.location.href : href);", el);
\endtt

Previous example is simple, yet if we wanted to cover it with our scenario
implementation we would bring a lot of single-problem-specific syntax into the
scenario. We would have to use special notation for obtaining current URL and for
conditioning on values. Following code demonstrates how this functionality
might look like if it was covered only by scenario syntax without usage of
JavaScript. The {\tt getCurrentUrl} function is inspired by Strigil. 

\begtt
{
  command: "condition", 
  condition: "ne", 
  param: "#", 
  value: {
    commad: "value-of", 
    selector: "a.detail", 
  }
  onfalse: {
    command: "function", 
    value: "getCurrentUrl", 
  }
}
\endtt

We have declared the {\tt condition} command with implementation of {\em ne}
the ``not equal'' operator (and for completeness we would implement all the 
other un/equality operators) and the {\tt function} command with implementation
of {\em getCurrentUrl} which, again, probably is not the last function to be
implemented. All this would require update of the scenario parser, the
implementation for commands and all their attributes and thus update of the
whole backend every single time, new functionality is needed. The advantage of
this approach is that user does not have to know JavaScript and understand how
it is called in WebDriver in order to use advanced conditioning and/or value
formating.

It is disputable if a set of extra commands in scenario syntax and hence extra
controls in scenario editor would be more understandable than a single field
for JavaScript function. Technically by adding conditioning and function
commands, we are inclining towards building a new programming language. To
offer the best for the user, implementing both is the option: basic
conditioning to easily direct the scenario flow along with a set of functions
to format and modify string and other values as well as enabling JavaScript
execution for complex problems. 

With use of JavaScript the same scenario step as in previous example would look
as follows: 

\begtt
{
  command: "value-of", 
  selector: "a.detail", 
  exec: "var href = elem.getAttribute('href');" +
        "return (href ==='#' ? window.location.href : href);"
}
\endtt

In this case we embedded only the {\tt value-of} with a single attribute that
takes JavaScript function body. From there we have technically unlimited power
for extending the functionality of the crOWLer without need of changing the
Java implementation. 

Note that compared to example in Java the first line of the original JavaScript
was omitted: 

{\tt var elem = arguments[0];} 

It will be automatically prepended every time, we exec JavaScript on a single
DOM element. It is a simple helper and does not invalidate any users input (as
in JavaScript we can redefine variable as many times as we want). Similarly we
will predefine variable {\tt elems} when a list of elements is passed, {\tt
value} when passing a string or number to a JavaScript function. 

But with great power comes a great current squared times
resistance~\urlnote{http://www.xkcd.com/643/} XXX. With usage of JavaScript as
suggested in previous paragraphs we have to take in account two major
considerations. 

Firstly, JavaScript function can accept any number of parameters and return an
arbitrary value. In both cases, the parameters and the return value can be
of any of the allowed types (as JavaScript is not strongly typed language). We
thus have to specify what exact parameters are being passed to a function and
what result of what type is expected. We also have to implement a robust way of
controlling this specification and properly define a fallback-on-error
behavior. This is especially important as we might want to use JavaScript
function not only as a string filter, but also for example as a universal
selector where we struggle with classical selectors. Any additional use have
to be described separately before it can be universally used. 

More importantly, there is the second consideration. Any DOM element is
accessible from any JavaScript function using for example the {\tt
document.getElementByTagName} method. When an element is replaced or even
removed, it becomes invalid from the Java context. Modification to an element
can cause unexpected behavior of its reference in Java too. The same applies
for operations on the whole page. When a link is followed, the original DOM
tree is dropped and all references are lost. 

To better describe the underlying behavior during this issue, below you can see
a simple test.  When link is clicked, the WebDriver follows the link in current
window and the reference to the original DOM is lost. 

\begtt
WebDriver wd = new FirefoxDriver();
wd.navigate().to("http://www.inventati.org/kub1x/t/");
WebElement a = wd.findElement(By.cssSelector("a"));
System.out.println(a.getText()); // Prints "detail"
a.click();
System.out.println(a.getText());
// throws org.openqa.selenium.StaleElementReferenceException:
// { "errorMessage":"Element does not exist in cache", ... }
\endtt

The previous can be partially solved by sandboxing the code in a closure. By
doing so we can hide some essential object in global scope like {\tt window} or
{\tt document} and make it harder to do inappropriate operations on the DOM.
In following example we create the described partial sandbox: 

\begtt
JavaScriptExecutor exec = (JavaScriptExecutor)driver;
WebElement elem = driver.findElement(By.css("div.wewant"));
exec.executeScript(
"return (function(elem, window, document) {" +
funStr +
"})(arguments[0])", elem);
\endtt

This technique is not completely secure (for example the element passed as an
argument does have reference to its parent which is already leak of intended
sandbox). Proper sandboxing would require implementing whole JavaScript engine
in JavaScript \cite[jsjs] which is probably too much for our intentions. 

~

In crOWLer, we can now distinguish between two ways of ascending to another HTML page: 

\begitems \style n
  * using {\tt call-template} command
  * using JavaScript or user event such as ``click'' or ``back''
\enditems

The {\tt call-template} is always called on an URL and always creates new web
context, keeping the original one untouched. It actually behaves like call
stack, so when we return from the template call, we can follow on the original
DOM tree. Just to note: compared to corresponding Strigil command, crOWLer
persists the ontological context throughout this call, and so we can relate to it
when assigning properties. 

Direct interaction with current window in any way that changes page location
will, however, irreversibly invalidate all the elements of current DOM. This
does not have to mean we can not use this functionality all together. Probably the
best solution would be to only allow DOM modifying operations on the bottom
level of templates (i.e. within the {\tt steps} property of the {\tt template}
command in scenario). At this place we only hold the {\tt body} of current
document and as such we can simply replace it with the newly loaded content. 
In the original crOWLer implementation, this would be the spot between two
``Initial Definitions''. 

Even thought the JavaScript is sandboxed in WebDriver, it is still running in a
browser in your computer and could technically submit some data on a web.
Security issues have not been considered so far, but might become a point of
interest when we take in account an option of obtaining and executing scenarios
from unknown sources. 


\sec User Interface

Here the required structure of user interface is described. 

\secc SOWL user interface

The user interface of SOWL shall be presented in a form of sidebar. The sidebar
shall have two parts: a scenario editor and resources list. Scenario editor
shall contain a tree shaped structure of steps of the scenario being created
along with panel for editing the general settings of the scenario. The
resources list shall accept dropping of ontology files which would load it into
current dataset. Addition of resources manually shall be possible using a
button. The list shall show all currently loaded resources and allow textual
filtering. 

SOWL shall enable tag selection on the webpage being processed by clicking or
other user action. 


\secc crOWLer user interface

CrOWLer is a console application. It shall accept scenario as one of its parameters. 
Following settings shall be enabled using parameters as well:
\begitems
  * setting of target directory for RDF files
  * setting of sesame repository for the result storage
\enditems


\sec Model

Presenting proposed design of the two programs the SOWL Firefox addon and the
crOWLer Java application. 

\secc SOWL model

Current recommendation of Mozilla Developer Network suggests developing new
addons using their native SDK. It allows creation of restartless addons, uses 
new API and limits usage of older libraries or low level calls by wrapping it 
in consistent API. 

The SDK based addons have partially predefined structure. The {\em background script}
runs in its own scope and uses the SDK API to control the addons behavior.
The {\em content script} is a JavaScript code that is injected into a webpage
but runs in its own sandboxed overlay, while having access to pages DOM and
JavaScript content. In SOWL, the scenario editor will be placed into a sidebar.
Sidebar holds standard HTML window object in which the JavaScript code is
running.  All three components communicate via textual messages using {\tt
port} object offered internally by by Firefox. 

\midinsert
\picw=12cm \cinspic sowl-components.png
\caption/f A component structure of the SOWL Firefox addon. 
\endinsert


\secc crOWLer model

In the new implementation of the scraping backend the original JSOUP component
will be replaced by WebDriver. WebDriver, with its support for JavaScript,
will help to handle dynamic content and brings in new possibilities for the
crOWLer itself. The original configuration component is replaced by parser for
the SOWL/JSON scenario format. The core crOWLer is also reimplemented
according to new set of instructions (i.e. commands in the scenario) and the
new web interface (i.e. the WebDriver instead of the native Java Jsoup
library). 

The overall architecture then looks as follows: 

\midinsert
\picw=7.5cm \cinspic crowler-new.png
\caption/f A new overall architecture of the crOWLer implementation. 
\endinsert


%TODO
%\sec Issues - solved and unsolved
%
%\begitems
%  * error handling (non existent selector, missing data, ...)
%\enditems

% * diagramy... clas, sequence (komunikace s jOWLem, komunikace s generatorem selectoru, ), 
% * model
% * vyresene, nevyresene problemy

