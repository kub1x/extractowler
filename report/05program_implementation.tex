 
\chap Program Implementation

\sec Libraries XXX

\secc rdfQuery

rdfQuery is a javascript library for RDF-related processing. It supports RDFa,
RDF, OWL for parsing data, it can dynamically embed HTML webpage with RDFa
data. rdfQuery is written in similar style as jQuery, popular Javascript
library. The intended use is to write queries over data stored in rdfQuery
internal datastore in similar way as DOM object are queried using jQuery.
Moreover the whole concept is based on SPARQL keywords, taking the best from
each world XXX. 

\picw=1cm \cinspic rdfquery-logo.png

\url{https://code.google.com/p/rdfquery/}


\secc aardvark

Aardvark is a javascript engine for in place modifications of a webpage. It
allows user to select, delete , or highlight part of HTML page. In its production version a


\sec Scenario format

One of main tasks of this work was to create format for scenario generated by
SOWL and consumed by crOWLer. This scenario will describe information necessary
for the crawling process: what operation to do (create ontological object,
assign property to such an object, perform task with webpage). 

This task is closely related to implementation peculiarity of semantic crawler:
we're dealing with two separate contexts at the same time, the ontological and
the web context. Ontological context holds current object (individual) to which
we assign properties, web context hold current webpage along with currently
selected element on that webpage. Scenario have to support operations to change
each context separately and/or both at the same time. 



\secc Strigil/XML

Scrigil, the scraping platform in order to solve similar problem as crOWLer
introduces it's own XML based Scraping Script format. It's documentation can be
found here XXX
\urlnote{https://drive.google.com/file/d/0B4On-lGb38CgWlAyZDhGbDV2TFk/edit
Scraping script documentation}. 

Basis of the whole script is system of {\em templates}. Each template has a name
and mime-type declaring type of document the template is designed for. This
information is needed as Strigil supports HTML and also Excel spreadsheet
files. Templates call each other using {\tt call-template} commad anywhere in
the script. This command accepts URL as an argument from it's nested commands.
Each template is called only with new URL, thus on new document. Of course URL
of current document can be passed as an argument, but due to nature of Strigil,
this would create completely separate context. 

Strigil is tailor made for parallel processing. The architecture of the Strigil
system contains not only scraping processor, but also a layer for distributed
download queue processing and layer of proxy servers that can be used to spread
the traffic and scale the download process horizontally. As the downloads are
performed asynchronously and can be even delayed due to network lags and
timeouts, there is no guaranteed order in which documents will be scraped.
Each of Strigil templates create it's own context when called. If we want to
link data obtained from different template calls we have to use some additional
techniques. For example we can assign some properly defined, non-random, unique
identifiers to an object. This identifier have to be quaranteed to be the same
for the same object through different template calls and potentionally on
different pages. 

To handle ontological data manipulation the commands {\tt onto-elem} and {\tt
value-of} are used. First one creates an individual of given type and, if
nested into different {\tt onto-elem} relates this new individual to it's
parent with some property. Literals are assigned to properties of parent object
using {\tt value-of} commad with property name specified. This command is very
powerful with usage regular expressions, selectors or nested calls of itself it
can create arbitrary values from constants and data obtained from web page
being processed. 

Strigil also implements variety of functoins to help with processing of textual
data. Function {\tt addLanguageInfo}, for example, is widely used in Strigil
scraping scripts to add language tags to string literals. The function call can
be seen below. 

\begtt
<scr:function name="addLanguageInfo">
  <scr:with-param>
    <scr:value-of select="Hello World" />
  </scr:with-param>
  <scr:with-param>
    <scr:value-of text="en" />
  </scr:with-param>
</scr:function>
\endtt

Similarly we can use function {\tt addDataTypeInfo} to add datatype flag,
function {\tt generateUUID} to obtain unique identifier or function {\tt
convertDate} to convert Czech and English dates into a common {\tt xsd:date}
format and several others. Some functions, like the last one mentioned, cover
task-specific issues and Strigil doesn't define a way to extend the list of
functions. 

In early stages of SOWL developement an attempt was made to use original
Strigil/XML as a format of choice. An appropriate, consistent subset was chosen
that would cover required use cases. Implementation of simple use cases
revealed some pitfalls of this decision and revealed several suggestions for
improvements on the approach and the format itself. 


\secc Adaptation of Strigil/XML format

Strigil creates it's scraping script internally hidden under GUI and leaves
user unaware of it's actual content. It might still serve well, at least for
developers, to keep the script compact and easily readable. Addition of
language tag as seen in previous chapter, is widely used patter that polutes
the resulting stcript with unnecessary overload. Suggested improvemend would
separate this functionality into an extra attribute of the {\tt value-of} tag
named {\tt lang}.

The same suggestion can be applied to the data-type specification. Moreover
{\em implicit parsing} of known datatypes would not only simplify the scraping
script, but also help to clean and clear the resulting data. 

Let's imagine hypothetical scenario of two similar tables on one page
containing two sets of data in the same format. For such a case we would need
to define a template on subset of DOM and call it twice with different root
node. Creation of {\tt dom-template} and {\tt call-dom-template} tags would
solve this issue and would allow scenario creator to narrow down his focus to a
subpart of the scraped webpage. This would be particullarly usefull on
compicated pages with a lot of nested HTML. {\tt dom-template} and {\tt
call-dom-template} would be defined within a single {\tt template} tag and
unlike {\tt call-template}, they would {\em keep} the ontological context co
call of {\tt value-of} within {\tt dom-template} would assign a property to
individual created by {\tt onto-elem} wrapping the current {\tt
call-dom-template} call. 

The architecture of Strigil (distributed downloader) suggests that it uses
simple raw HTML pages as they were downloaded and uses JSOUP to extract data
from it as JSOUP is the selector system of choice. Many webpages, or even web
applications, make use of dynamic AJAX calls to fetch additional data after the
presentation layer of the web is shown to the user. Strigil doesn't handle
these cases by default. The internal AJAX code could be analyzed and simulated
using {\tt call-template} call, but this requires deep knowledge of the webpage
being processed. In crOWLer we opted to switch from JSOUP to WebDriver library
and use PhantomJs, a no-GUI web browser. This technology allow us to handle
webpages the same way as user sees them. 

Usage of actual full-stack web browser with javascript engine long with
WebDriver allows us to inject and execute arbitrary javascript code into the
processed webpage. In order to make full use of this feature we can define {\tt
function-def} tag which would define javascript function with name and params
and contain it's code. To execute this function we would call {\tt
function-call} and identify it by it's name. Return value of this function can
be then used the same way as the one from {\tt value-of} tag. 

From the experience with developement on Strigil/XML we can derive, that it is
tied with it's intended use for distributed downloader and it lacks some
functionality. In SOWL we would almost necesarily modify it's formal definition
and thus it is of consideration if we can't make use of more appropriate
format. 

% Strigil wiki on sf \url{http://sourceforge.net/p/strigil/home/Home}

\secc SOWL/JSON

As all Firefox extensions, SOWL is written entirely using javascript with
additional HTML defining the graphical layout. Early stages of implementation
generated XML based on Strigi/XML format using hardcoded XML snippets and
string formating -- approach often used on webpages with dynamically loaded
content. A string holds a snippet of HTML or XML structure with placeholder.
This placeholder is replaced by either a value or by another already processed
snippet. This way piece by piece the whole scenario is generated. This solution
isn't hard to implement, but brings in poor maintainability and with additional
complexity it looses elegance, readability and can even cause performance
issues. 

Original data of the scenario created by SOWL are stored naturally in
javascript object. Using sntadard javascript method {\tt JSON.stringify()} we
can immediately generate JSON serialization of such object. This way we have
structure similar to the original defined by Strigil/XML, but in flexible
structure. Obviously some adaptations are necessary. Nesting is recorded using
the {\tt steps}, the header section is redesigned for the JSON structure. XXX

The original semantics of {\tt onto-elem} and {\tt value-of} was preserved,
only limited to it's basic use. {\tt value-of} serves solely to assign literal
properties. 

The final scenario for Use Case 1 XXX looks like this: 

\begtt
{
  type: "scenario", 
  name: "manual", 
  ontology: {
    base: "http://kub1x.org/onto/dip/t/", 
    imports : [
      {
        prefix: "foaf", 
        uri: "http://xmlns.com/foaf/0.1/", 
      }, 
      {
        prefix: "kbx", 
        uri: "http://kub1x.org/onto/dip/t/", 
      }, 
    ], 
  }, 
  creation-date: "2014-11-30 12:40", 
  call-template: {
    command: "call-template", 
    name: "init", 
    url: "http://www.inventati.org/kub1x/t/", 
  }, 
  templates: [
    {
      name: "init", 
      steps: [
        {
          command: "onto-elem", 
          typeof: "http://xmlns.com/foaf/0.1/Person", 
          selector: {
            value: "tr", 
            type: "css", 
          }, 
          steps: [
            {
              command: "value-of", 
              property: "http://xmlns.com/foaf/0.1/firstName", 
              selector: {
                value: "td:nth-child(1)", 
                type: "css", 
              }, 
            }, 
            {
              command: "value-of", 
              property: "http://xmlns.com/foaf/0.1/lastName", 
              selector: {
                value: "td:nth-child(2)", 
                type:  "css", 
              }, 
            }, 
            {
              command: "value-of", 
              property: "http://xmlns.com/foaf/0.1/phone", 
              selector: {
                value: "td:nth-child(3)", 
                type:  "css", 
              }, 
            },
            {
              command: "call-template", 
              name: "detail", 
              attribute: "href", 
              selector: {
                value: "detail", 
                type:  "css", 
              }, 
            }, 
          ], 
        }, 
      ], 
    }, 
    {
      name: "detail", 
      steps: [
        {
          command: "value-of", 
          property: "http://xmlns.com/foaf/0.1/nickname", 
          selector: {
            value: ".nick", 
            type:  "css", 
          }, 
        },
      ], 
    }, 
  ], 
}
\endtt






