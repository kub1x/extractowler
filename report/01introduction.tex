\chap Introduction

During past few years the Web has undergone serveral bigger or smaller
revolutions. 

\begitems
  * WEB 2.0 and tag cloud
  * HTML5 and semantic tags
  * Smartphones, tablets, responsivity and mobile web everywhere
  * The run out of IPv4 addresses, nonexistent boom of IPv6
  * Cloud technologies and BigData
  * Bitcoin, Tor, anonymous internet 
  * WikiLeaks, NSA, Heartbleed and secourity concerns
  * Google Knowledge Graph, Facebook Open Graph, ...
\enditems

That's only few examples of some of the biggest recent technology booms and
issues on the global network. So little can mean so much in such a global
environment. The enviroment online is constanlty changing, usualy on a wave
of some new, useful or frightening technology or with popularization of a new
phenomena. The Semantic Web technologies have been described, standardized and
implemented for several years now~\fnote{ One of the most recent standards --
OWL2 -- was released in 2008 \cite[wiki:owl]} and their tide seems to be near,
though yet to come.

Semantic Web itself reates to several principles (along with their
implementation) that allow users to add meaning to their data. This meaning
brings not only a standardized structure, but also, as a consequence, the
posibility to query and reason on data originating from multiple sources. Once
given the structure, similar data can be joined in a form of a bigger bulk.
Presenting this data publicaly creates a virtual cloud. This phenomena is
called Linked Data. 

In this work we'd like to bring the Semantic Web technologies closer to users.
We will propose a methodology for extracting and annotating data out of
unstructured web content, along with design and implementation of a tool, to
simplify the process. Results will be confronted with real life use cases. 


\sec Problem Statement and Motivation

% TODO chceme oanotovat konkretni stranku konkretni ontologii
% TODO v dusledku chceme obecne zjednodusit pridavani vyznamu datum na webu

Giving meaning, i.e. semantization of web pages gets more popular. Probably the
most obvious example can be seen in the way the Google search engine serves
it's results. Presenting not only the resulting pages but as well snippets of
information scraped directly from the page content such as menu fields parsed
directly from CSS annotation or HTML5 tags, contact information or openning
hours, or even visualizing data from their own internal ontology, the Knowledge
Graph~\urlnote{https://en.wikipedia.org/wiki/Google_Knowledge_Graph}. 

XXX Strigil - \url{http://delivery.acm.org/10.1145/2540000/2539170/p453-starka.pdf}

What options do we have to bring sematic into a webpage? 

%In future they're going even beyond that and suggest "predictive search"
%\url{http://www.nytimes.com/2013/07/30/technology/apps-that-know-what-you-want-before-you-do.html}
%technologies, that try to guess the motivation of your search, not only it's
%meaning. \url{http://www.google.com/landing/now/}

One direction to go is to annotate data on "the server side", i.e. at the time
it is being created and/or published. The person or engine creating the data
have to use the right tool and put some time and effort giving the data the
appropriate annotation. There are standards covering this use case. HTML5
brings in tags for clearer specification of the page structure (such as {\tt
nav}, {\tt article}, {\tt section}, {\tt aside}, and others). Microformats
\url{http://microformats.org/} define specialized values for HTML {\tt class}
atribute to bring standardized patterns for several basic use cases with fixed
structure, such as {\em vCard} or {\em Event}. The microformat approach is easy
to implement as it doesn't impose any extra syntax an can simply embed an
existing page source. Last but not least, we can use RDFa to annotate data on a
webpage with an actual ontology. This technology is part of the Semantic Web
stack and we'll describe it closer in further chapter (TODO link).

Annotating data on the server side enables users to use tools to highlight data
they are specifically interested in, extract them and reason on them. Services
can use annotated data, combine them and offer results from multiple sources. 

Some examples of utilities for extracting and testing or scraping structured data: 

\begitems
  * \url{http://www.google.com/webmasters/tools/richsnippets}
  * \url{http://rdfa.info/play/}
  * \url{https://code.google.com/p/ldspider/}
  * \url{http://ldodds.com/projects/slug/}
\enditems

To bypass the gap between unstructured data present on the web on one side and
rich, linked, meaningful ontologies on the other, we can go the opposite
direction as well. We can take the unannotated data already present on the web
and retrieve them in a form, that is defined by some ontology structure. This
process can be performed either automatically or manually. 

When comming to the automated crawling, we're mostly interested in improving
ranking of search results. Ontology is used to help crawler to find relevant
pages to a keyword or to finetune the ranking
metrics~\urlnote{http://www.researchgate.net/publication/220830610_An_Ontology-Based_Crawler_for_the_Semantic_Web}. 

%There've been several attempts in the automatic data annotation. In
%principle, the goal of this approach is to automatically analyze a web page,
%find the most appropriate ontology for it and use this ontology to
%automatically annotate data on the page. 

%As an example, on a webpage of a music band this would find (XXX
%http://www.musicontology.com/) as the best matching schema, and it would
%annotate band name, genre, albums using classes and  properties from this ontology. 

While on well structured, simple and/or partially annotated pages this process
can be very successfull and produce usefull results, on pages with unorganized
data the confidence on results produced by this approach might drop to minimum. 
Unfortunately many online webpages and services are poorly strucruted. Pages
containing many unrealted data, in form of advertisements or other external
content might confuse such an engine. Old servers present their content in
poorly structured or even invalid HTML usually in a form of multiple nested
tables that serve for structuring only and makes the whole structure of the web
unreadable. Social aspect of web brings in almost complete randomness
making it even harder to automatically reason on page's content if it can't be
distinguished and potentially left aside. We've mentioned few, both potential
and real threads that prevent us from automatically annotate all data on web
with confidence. 

In some use cases the ontology of the desired data is yet to be created and the
user is aware of the data strucrutre and capable of manually spot and select
the data on a web page. Currently there isn't many tools allowing this kind of
operation. The ideal implementation and the vision here will allow user to
partially identify the structure of a webpage while leaving the repeatative
tedious work on crawler following the same procedure repeatedly on all data of
the page. 

For such a proccess we need to create tools that allow users to address
previously unstructured content, link it to resources of existing ontology
and/or create these resources on-the-go. By using existing ontologies we would
not only give the meaning to our data, but also create valuable connection to
any other dataset annotated using the same ontology. 


\sec Current solution crOWLer

The suggested base-technology is being developed on our faculty. Crawler called
crOWLer serves the needs of extracting data from web. It follows the workflow
of scraping data using manually created scenario with given structure and
user-defined set of ontological resources. 

In previous implementation, both, the scenario, followed by the crawler, and
the ontology structure/schema are hard-coded into the crOWLer code. This
requires unnecessary load of work for each separate use case, whilst in
practice all the use cases share the same workflow. 

\begitems \style n
  * load the ontology
  * add selectors to specific resources from the ontology
  * implement the rules to follow another page
  * run the crawling process according the above
\enditems

In the initial crOWLer implementation it is necessary to fullfill the first
three steps with an actual programming. In order to perform this task, we need
to have a programmer with knowledge of Java programming language, and several
technologies used on the web, along with knowledge of the domain of data being
scraped in order to correctly choose appropriate resources for annotation.
There is also a huge overload in preparation of development environment and
learning time of the crOWLer implementation. The need of more elegant and
generic solution is evident. 


\sec Proposed Solution and Methodology

To simplyfy the creation of guidelines, or scenarios for crOWLer, we propose a
tool that allows user to select all the element directly on the web page being
crawled, with all the necessary settings, pass the scenario created to the
crOWLer and obtain the results in a form of RDF graph. 

\sec Specific goals of the project

\begitems
  * design the semantic data creation use-cases
  * create syntax for scenario for crOWLer 
  * implement a web browser extension for creating scenario for crOWLer
  * this extension shall
  \begitems
    * load and visualise ontology 
    * join page structure and ontology resources in a form of scenarion
    * serialize scenario and necessary ontological data
  \enditems
  * parse the scenario by crOWLer
  * run crOWLer following the scenario
  * visualize the extracted data (feedback)
\enditems


\sec Work structure

Next part of this work will cover tools and technologies related to the work.
Chapter XXX will describe research on existing solutions and how they
influenced results of this work. XXX program design. XXX program
implementation. At the end we'll evaluate results of this work against proposed
use cases. 


