\label[chap:research]
\chap Existing solutions % and Research

In this chapter we will describe the research on existing solutions for
given task (scraping and annotating data from a web). The performed search was
focused on tools directly targeting the problem, as well as libraries and
technologies that could be included in the solution or existing open source
programs we could build the solution on. 


\sec Semantic and non semantic crawlers

By researching existing solutions, there is currently no open source or openly
available solution that would directly follow the required workflow and
fulfill the requirements. 

Existing tools named as ``Ontology-based Web Crawlers'' refer mostly to crawlers
that rank pages being crawled by guess-matching them against some ontology.
In those programs user can not specify data that are being retrieved. Moreover,
there is no way to get involved in the crawling process. The tool is solely
used to automatically rank the relevance of documents which solves different
set of problems. 

In the case we are trying to solve, the input is one or more documents and one
or more ontologies. The result is data retrieved from the documents and
annotated with resources from the ontologies. 


\secc Advantages and pitfalls of Semantic crawlers

To properly target the benefits the semantification of the scraped data brings
to the user, let us quickly follow an evolution from the most primitive
technologies for scraping data to the advanced ones. The ultimate goal is to
effectively search in data and maximally utilize the knowledge it carries. 

The simplest approach is manual searching for keywords, or even simple browsing
the web. That might be useful in some cases, but when there is a lot of data,
it becomes exhausting. 

Crawling data using simple tools like {\tt wget --mirror} allows us to load data
and then write a program or script to retrieve a relevant information. This
approach takes a lot of energy for one time only solution of a given problem. 

By storing such crawled data into database we obtain persistent database,
possibly automatically obtained by the script from previous case. Such data is
static, but can be queried over and over and possibly re-retrieved when it becomes
obsolete. Its structure is, however, based on programmer's imagination and
needs to be described in order to understand and handle the data properly. 

When a triple store is used as the database in previous case we obtain one-time
solution to our problem. This is technically equal to original state of
crOWLer. 

When using Ontology-based solution, tailor made for crawling and annotating
data from web, we obtain several benefits ``for free''. The tool designed
specially for this purpose makes it easy. Once the data is annotated, we can
not only query on them, but also automatically reason on them and obtain more
or more specific/narrow results than with general data. The attributes and
relations within ontology that allow reasoning, are usually part of the
ontology definition and as such comes in naturally without any extra effort. 

Last for benefits: using ontology from public resource as a schema for our data
can give us correct structure without need for building it from scratch. Also
by using some common ontology, we can join together any accessible data
structured according to this ontology and simply query on resulting super set.

~

However, semantic crawling is not a silver bullet yet. This technology is still
finding its place and uses and is constantly shaped by the needs of the users.

For instance there is always a threat of inconsistency of an ontology when some
data do not fit the rules or break structure of an ontology. In its state from
April 2014 DBpedia states, there is 3.64 million resources, out of which 1.83
million are classified in a consistent Ontology \cite[dh:dbpedia]. That is only 
half of the data being arguably consistent with each other. This does not mean 
that the rest of the data is bad; however it might cause a inconsistency and
prevent us from reasoning on the data if we include a wrong subset of the data. 

Just like with hardcoded crawling technique, the semantic crawling is
tightly bound to the structure of the crawled web. The web is being matched
against some pattern described by selectors and the matching element, when
found, is accepted for further processing. Any change on a webpage structure
can lead to broken selectors or links during the crawling process and make the
scenario partially or completely invalid. 

Many web pages load their data dynamically using AJAX queries. Some pages
simply change their content frequently (e.g. news pages, forums, user content
pages, like video or music servers and social web applications). Crawling
content on such servers would require almost constant crawling and would cause
growth into massive ontology of, oftentimes, questionable quality. 

The semantic crawling is an useful way to effectively obtain and query on
data from the web, but it still have its challenges to overcome. 


\label[sec:crowler]
\sec Analysis of crOWLer

A thorough analysis of the current program shall precede creation of the final
design. We will focus on architecture, dependencies and components that will
have to be reimplemented. 

\midinsert \clabel[pic:crowler-orig]{General architecture of the original crOWLer implementation}
\picw=10cm \cinspic crowler-orig.png
\caption/f General architecture of the original crOWLer implementation
\endinsert

In original implementation crOWLer is a prototype of console Java application.
It uses Apache Jena library~\cite[jena] for handling ontological data and JSOUP
library~\cite[jsoup] for accessing webpages and addressing elements. Instead of
scenario file, crOWLer accepts Java {\tt .class} files containing an
implementation of {\tt ConfigurationFactory} class. This factory class builds a
{\tt Configuration} object. In
appendix~\ref[app:crowler-old-configuration-class] you can see definition of
classes defining the configuration component of crOWLer. The class diagram in
appendix~\ref[app:crowler-old-selector-class] describes the {\tt
InitialDefinition} and the {\tt Selector} classes that are main building blocks
of configuration. Configuration defined using this structure specifies all the
information needed for crawling process: 

\begitems
  * webpages to be crawled in a form of list or pagination description
  * way to address data on each page using JSOUP selectors
  * definition of ontology resources used to annotate the obtained data
  * setting of how URI will be created for each individual
\enditems

\midinsert \clabel[pic:crowler-old-core-class]{Core classes of original crOWLer implementation} 
\picw=10cm \cinspic crowler-old-core-class.png
\caption/f Core classes of original crOWLer implementation
\endinsert

The main program flow of crOWLer lays upon few core classes. The pair {\tt
FullCrawler}, {\tt Crowler} (diagram \ref[pic:crowler-old-core-class]) form the
crawling process loop. In this loop {\tt FullCrawler} fetches the source web
pages and passes them one by one to the {\tt Crowler}. The {\tt
NextPageResolver}, which defines list of pages to be crawled is structure
implemented within the configuration and thus is specific for given problem
instance. Results are stored in the outer loop after each scraped page. According
to input parameters data are uploaded into Sesame repository using {\tt JenaSesame} 
library, or locally in an RDF file. 

The inner loop performed by {\tt Crowler} finds a set of HTML elements as
defined by the {\tt InitialDefinition} class. Each of these
elements serve as a root for a tree of ontological individuals linked by their
properties. The tree is in configuration defined
using {\tt ClassSpec} and {\tt PropertySpec} classes that hold definition of
type of the individual and the assigned property respectively. The spec-classes
also carry information about selector, used to find the corresponding HTML element. 
A collection of {\tt Selector} classes is available and can be extended.  JSOUP
selector handling is implemented as well as selector chaining or resolving data
from a link target. 

In Crowler an individual of an ontological object is created after all his
defined properties values are scraped within the inner loop, as the URI of the
individual can be formed using one or more of these values. This way we can
refer to the same object if we create individual of the same URI on two
different pages for example. 


\secc Issues of crOWLer configuration

From deeper analysis of the original crOWLer source we can observe that the
whole scraping process relies on the configuration defining it -- a set of Java
classes, implementing the predefined interfaces and using the API provided.

This reveals the issue being addressed. Writing a crOWLer configuration
requires knowledge of Java programming language along with knowledge of RDF
technologies. Programmer also gets into the position of ontological engineer
when designing ontological resources used in the configuration. Knowledge of
WEB technologies is needed in order to properly target elements on the webpage
using JSOUP selectors. This is one of the hardest task as the selectors have to
be manually extracted using for example browser console. 

The scenario based approach, focused in this thesis, will enable user to
bypass the Java programming and focus only on matching web structure with an
ontology. 


\secc Confrontation with use cases -- technical issues

In this section the capabilities of the original crOWLer implementation will be
confronted with use cases specified for this work \ref[sec:usecases]. 

For all use cases a separate configuration would have to be created. We will 
mainly focus on problems specific for each case. 

~ 

The first configuration of crOWLer was created for the {\em Monumnet} webpage
of the National Heritage Institute, the UC2~\ref[sec:intro:uc2]. Stating that
the UC2 can be and was solved using the hardcoded configuration. 

First we will focus on the structure of configuration. Following code is a
simplified snipped of actual configuration building code of original crOWLer
implementation. It uses {\tt NPU} class as simple static storage for URIs used
in our ontology. According to this configuration a {\em monumnetRecord} object
is created for each table row as defined by the {\tt initialDefinition}. The
second part creates {\em district} object with its label (found in third table
column denoted by the {\tt td:eq(2)} JSOUP selector) and assigns it to the
record using {\em hasDistrict} object property. The {\tt conf} object holds the
configuration being passed to the actual crawler. 

\begtt
ClassSpec chObject = Factory.createClassSpec(NPU.monumnetRecord.getURI());
conf.addInitialDefinition(
       Factory.createInitialDefinition(
         chObject,
         Factory.createJSoupSelector("table tbody tr.list")));

ClassSpec sDistrict = Factory.createClassSpec(NPU.district.getURI());
chObject.addSpec(
           Factory.createOPSpec(
             Factory.createJSoupSelector("td:eq(2)"),
             NPU.hasDistrict.getURI(),
             sDistrict));
sDistrict.addSpec(true, Factory.createDPSpec(Vocabulary.RDFS_LABEL));
\endtt

This pattern is, with some variation, repeated for all data properties and
object properties. The interesting part is how crOWLer handles the detail page
link. Just to remind a situation in UC2~\ref[sec:intro:uc2], each table row of
the page uses unique {\tt onclick} attribute in following form: 

\begtt
document.listpf.IdReg.value='131164'; document.listpf.submit();
\endtt

The numerical value {\tt IdReg} corresponds to last column of the row and holds
the identification number of the national monument in the MonumNet system. 
As crOWLer handles every page as a static HTML document, there is no way to 
execute this code as a JavaScript handler. Instead it is being parsed by a
regular expression and the result is used to fill in a format-string creating a
URL. This URL locates the detail page for each table record. 

\begtt
Factory.createNewDocumentSelector(
  conf.getEncoding(),
  Factory.createAttributePatternMatchingURLCreator(
    "onclick", 
    ".*([0-9]+).*", 
    MONUMNET\_URL+"pamfond/list.php?IdReg=" + "{0}"));
\endtt

Technically this is a form of a workaround, rather than systematic solution 
of the given problem. We can not securely rely on JavaScript code within the
attribute as a part of data. It is important to realize, that the technique
used on the webpage is rather non-standard and can not be effectively covered
with general purpose tool without a need of problem specific solution. 

~

Understanding the configuration implementation we will now briefly analyze the
rest of use cases. crOWLer would solve the UC1 \ref[sec:intro:uc1] with quite
basic configuration. Here we present a short example: 

\begtt
ClassSpec chObject = Factory.createClassSpec("foaf:Person");

conf.addInitialDefinition(
       Factory.createInitialDefinition(
         chObject,
         Factory.createJSoupSelector("tr")));

// First name
chObject.addSpec(
           Factory.createDPSpec(
             Factory.createJSoupSelector("td:eq(0)"),
             "foaf:firstName"));

// Analogically for the rest of properties

// Link to detail page
chObject.addSpec(
  Factory.createDPSpec(
    Factory.createChainedFirstElementSelector(
      Factory.createNewDocumentSelector(
        conf.getEncoding(),
        Factory.createAttributePatternMatchingURLCreator(
          "href", ".*", KUB1X\_URL + "{0}")),
      Factory.createJSoupSelector(".nick")), 
      "foaf:nickname"));
\endtt

This example is using only classes from the original crOWLer. Note at the
bottom How we define following a link to the detail page. In proper
implementation we would probably simplify the new document selector creation by
wrapping it in a single factory method {\tt createLinkTargetSelector}, which
would internally create selector for the address targeted by {\tt href}
attribute of the link tag either absolute or relative to current document (so
that we could avoid the explicit specification of URL using {\tt KUB1X\_URL}
constant. 

If we wanted to get more properties from the resulting page, we would reuse 
the {\tt NewDocumentSelector} in combination with selector targeting value of
each property. crOWLer always relates selectors to the document currently
referenced by the outer loop in the {\tt FullCrawler}. Whenever a selector
containing {\tt NewDocumentSelector} is applied during the crawling process, a
REST call is performed to fetch the targeted document. On a MonumNet webpage
this means hundreds of thousands of calls for each run of the crOWLer (over
40~000 records each with 16 properties on detail page). Caching system can be 
implemented to reduce this amount to the necessary minimum. We are still bound by
the double loop architecture though. 

The UC3 \ref[sec:intro:uc3] is equal to UC1 according to configuration complexity. 
All links are implicitly specified in a form of hyperlinks without any interruption 
or dynamic content change. Moreover in crOWLer configuration we can specify
what properties will, combined together, form URI of ontological object we are
building. This is exactly the additional functionality required by UC3. 

The specificity of fourth use case \ref[sec:intro:uc4], as described, lays in
AJAX driven pagination. Every ``page change'' event dynamically updates the 
content of the webpage. In this specific case we do not need to be alarmed
as the pagination component is created using jQuery DataTables
plugin~\cite[datatables]. Using this plugin the pagination is built on top of
the data table after it has been completely loaded. In case of crOWLer, the 
plugin is never executed and the table remains complete and unchanged over the
whole scraping process. 

This is not always the case, though. Even the DataTables plugin itself supports
loading data through AJAX so the alertness is more than appropriate. In the
hypothetical situation when AJAX is used for data loading crOWLer would not be
able to handle the pagination and would only access the first page.  The
additional data would have to be loaded using workaround similar to the one in
UC2.  And even if we successfully load the data we still might be unable to
handle them by crOWLer. The AJAX call typically serves only the new chunk of
data to be inserted into the page either in HTML or in JSON format. When in
HTML, we would have to extend the configuration to correctly target elements in
the reduced form of AJAX update. In case of JSON a completely new selector
system would have to be added to crOWLer. 

The situation dramatically changes if we use full stack web environment with
JavaScript engine. In that case we would be able to ignore the background 
functionality of pagination and simply simulate click on the ``Next'' button. 
Enabling JavaScript has huge consequences and will be analyzed in a separate
section \ref[sec:javascript]. 

\secc Result form crOWLer analysis

The original implementation of crOWLer can solve tasks defined by specified use
cases~\ref[sec:usecases]. The requirements on users of crOWLer are too high and
the usability is very limited. The options of extending the configuration
component will be examined during the design part\ref[chap:design]. The
configuration can be either generated using scenario or completely replaced, if
the scenario defines different crawling procedure (other than current double
loop). The option of incorporating JavaScript will get an extra attention. 

Previous section roughly define requirements on scenario for semantic crawler.
To fully satisfy all considered use cases in all settings, in addition to the
functionality implemented so far, we would have to cover the: 
\begitems
  * following hyperlinks on a page, 
  * firing JavaScript and browser events, 
  * functions of transforming scraped data using regular expressions or key--value mapping. 
\enditems


\label[sec:strigil]
\sec Strigil

Strigil is an ontological scraping system developed at  Faculty of Mathematics
and Physics of the Charles University in
Prague~\urlnote{http://xrg.ksi.ms.mff.cuni.cz/software/ld/ldi.html\#strigil}.
It represents an easily configurable tool that enables users to retrieve data
from textual or weak structured documents. \cite[iiwas2013:strigil]

\midinsert \clabel[pic:strigil-overall]{Overall Architecture of Strigil}
\picw=10cm \cinspic strigil-overall.png
\caption/f Overall Architecture of Strigil
\endinsert

It consists of {\em Data Application} in a form of webserver and backend
service providing {\em Download System} for the application. The webserver
offers frontend for configuring the crawling process. The application then
follows the configuration scraping the data and storing results while using the
backend handle downloading. Strigil strongly focuses on the download process.
Components of the backend conform in a structure of {\em DownloadManager}, {\em
Downloader}s and {\em Proxy} servers that help to distribute the load of data
being transfered. 

The frontend part serves user interface for handling ontological data on top of
a web being scraped. It internally creates a scraping script (will be
referred to as Strigil Scraping Script or Strigil/XML) which strongly inspired
format for scenario used in the actual implementation later in this work and
will be closely analyzed in chapter \ref[chap:design]. 


\secc What problem does it solve?

The architecture of Strigil (more in \ref[app:strigil-architecture]) is tailor
made for parallel processing of documents. The installation of Strigil requires
working Apache2 web server with PHP5, Tomcat, PostgreSQL database, OpenMQ
service and several other components before the actual deployment of Strigil
into the environment. The system is designed for processing many requests on
targeted server, heavy loads of data and long running tasks. Its complicated
architecture and installation process prevents it from being effectively used
in occasional simple, yet non trivial, scraping tasks. 

Moreover its download system fetches only the raw HTML data (just like the
original crOWLer implementation) and treats it as static document. This way it
can not properly handle dynamic content and temporal changes in documents
performed by JavaScript for the exact same reasons that applied for crOWLer. 


\secc Strigil vs crOWLer

Because of the difference in complexity of Strigil and crOWLer, we can't
correctly compare them one to one. But we might find a common subset of
functionality. Strigil is a server, with frontend, scraping unit and download
system. crOWLer is a tool without user interface and with download system
reduced to simple REST calls. The common part then is the scraping unit. 

The scraping algorithm of crOWLer has been described previously in
section~\ref[sec:crowler]. It consist of outer loop over documents, inner loop
over initial definitions and tree of recursive calls forming the ontological
structure while scraping data from elements on the page. 

Strigil has a slightly different approach. Instead of configuration it is
guided by a scraping script. The script will be closely analyzed in the
following chapter, but in general it defines a set of templates where one
template is called at the beginning and each template can call any other
template on some URL (i.e. on document located by the URL). Unlike in crOWLer
the processing of each template is performed independently in Strigil. Each
template call puts a request into the download system first. The actual
execution of each template is fired asynchronously, when download of the
targeted document is finished as notified by message from the Download System. 

The inner part of a template conforms with structure inside crOWLer
configurations. It defines tree structure of ontological classes and
properties along with selectors specifying the position of targeted data. 
Resulting from different document resolving system, there are no {\tt
NewDocumentSelector}s in Strigil. In place of this selector, we would
simply call another template on the new document. This approach is clearer than
using chained selector, especially if we handle two or more nested documents.
It is required though, to carry the ontological context from one template to
another. This behavior is unfortunately neither mentioned in Strigil
documentation, nor in examples examined. 


\secc Confronting Strigil with use cases

As a basic example UC1\ref[sec:intro:uc1] can be solved by Strigil. We are
presuming here that strigil carries the ontological context through template
calls. Notice in the following example that the {\tt value-of} tag in the
template named ``detail'' does not have any {\tt onto-elem} defined above it.
By carrying the ontological context we denote that every property specified by
the children nodes of the {\tt template} will be assigned to individual created
by {\tt onto-elem} containing the invoked {\tt call-template}, i.e. the
property assignment will bubble through the template call until it finds an
{\tt onto-elem} node.  Unfortunately Strigil documentation does not state this
clearly and the examples provided do not contain the ontological context
carrying structure. 

\begtt
<scr:template name="init">
  <scr:onto-elem typeof="foaf:Person"
                 selector="tr">
    <scr:call-template name="detail">
      <scr:value-of selector=".detail @href" />
    </scr:call-template>
  </scr:onto-elem>
</scr:template>

<scr:template name="detail">
  <scr:value-of property="foaf:nickname"
                selector=".nick" />
</scr:template>
\endtt

Also it is important to note, that strigil uses JSOUP selector system 
extended by the attribute selector. In the example we target a value of
a {\tt href} attribute of elements with class {\tt detail}. The "@" tag 
is probably taken from XPath\cite[w3c:xpath]. This kind of extension 
is rather unfortunate as it combines two different syntaxes. As we 
primarily use JSOUP, the space in the selector sting denotes {\em any 
descendant}. In that case we would read the example selector as ``any href
attribute of any descendant of elements with class detail'' which probably is
not the intended meaning. We would suggest adding attribute named {\tt
attribute} to the {\tt value-of} element rather then extending the JSOUP
syntax. 

Strigil Scraping Script also does not allow the {\tt selector} attribute on the
{\tt onto-elem} element. There seems to be no other option, even for repetitive
patters on webpage such as table row, but to define the script on each one of
them. By allowing the {\tt selector} attribute we would bring in an intuitive
structure meaning ``create individual for each matching element''. 

For simplification we suggest implementation of these suggestions in following
analysis. 

~ 

Second use case \ref[sec:intro:uc2] would be solved in similar manner as in 
the hardcoded crOWLer solution, i.e. by extracting a value from onclick
attribute and manually building the target URL. 

\begtt
<scr:template name="init">
  <scr:onto-elem typeof="npu:MonumnetRecord"
                 selector="tr">
    <scr:call-template name="detail">
      <scr:function name="conc">
        <scr:with-param>
          <scr:value-of text="http://monumnet.npu.cz/?idReg=" />
        </scr:with-param>
        <scr:with-param>
          <scr:value-of selector="."
                        attribute="onclick"
                        regexp=".*(d+).*"
                        replace="{0}" />
        </scr:with-param>
      </scr:function>
    </scr:call-template>
  </scr:onto-elem>
</scr:template>

<scr:template name="init">
  (...)
</scr:template>
\endtt

~

In case of UC3 and UC4 the situation is practically identical for Strigil and
for crOWLer.

Just like crOWLer, Strigil natively supports setting of values used to create
identifier for an individual. In Strigil an URI of individual crated by {\tt
onto-elem} is specified by the first {\tt value-of} child node that returns a
value (i.e. does not have the {\tt property} attribute specified). In crOWLer
we can only specify what data properties will be part of the generated identifier
in Strigil we can create arbitrary URI using {\tt value-of} elements and
functions provided. 

Strigil does not handle AJAX calls and a workaroud would have to be implemented
for UC4. Just like crOWLer, Strigil downloads the raw HTML page and thus does
not even encounter the pagination widget present on the page. 

~

\secc What inspiration it brings for crOWLer

\begitems
  * The scraping script specifies the template system. Compared to loops in crOWLer
it appeals more natural and well structured. It also brings extra flexibility by
calling templates from within each other.

  * The XML format is, however rather verbose. Other, less verbose syntax might
serve better while persisting most of the semantic. 

  * The system of functions provided, gives a good set of tools for string
manipulation. Sometimes we encounter a problem-specific notion (e.g. function
for conversion of Czech and English date formats, rather than general use date
parser).

  * If the Strigil Scraping Script gets implemented in the crOWLer in some form,
the suggested improvements will be incorporated in the implementation as well. 
\enditems


\sec Finding platform for frontend

In order to develop appropriate tool for generating scenarios, several similar
tools were inspected for best practices, libraries, and possible extension. 

The resulted implementation is named SOWL (short for SelectOWL) and refers to
Firefox addon for creating scenarios for crOWLer. In following sections we will
refer to SOWL as set of requirements and a envisioned expected result of this
work. The actual implementation will be covered in later chapters. 


\label[sec:infocram]
\secc InfoCram 6000 -- ExtBrain

InfoCram 6000 is part of project ExtBrain \urlnote{http://www.extbrain.net}
that is developed on Department of Computer Science. This specific part
was implemented by Jiří Mašek and is described as ``prototype of user
interface for visual definition of extraction rules for ExtBrain Extractor''.
Its intended usage is very close to the usage of SOWL. It is an Firefox
extension that generates rules (scenario) for extractor implemented as another
part of the ExtBrain project. 

The ExtBrain extractor is implemented in JavaScript as opposed to Java in case
of crOWLer. It extracts data according to definitions by InfoCram 6000. The
result is stored in JSON format thus not carrying semantic information, but
only set of raw data in some form. 

\midinsert \clabel[pic:infocram]{Main Window of InfoCram 6000}
\picw=7cm \cinspic infocram.png
\caption/f Main window of InfoCram 6000
\endinsert

Main part of the extension window shows a tree view with rules being edited.
This view corresponds to required structure of scenario for crOWLer. 

Interesting part it an engine for selection elements of page. Its
implementation is based on
Aardvark~\urlnote{https://addons.mozilla.org/en-US/firefox/addon/aardvark/}, a
Firefox extension that addresses this issue using mouse selection and several
keyboard commands. 

InfoCram does not use simple CSS or XPath selectors, but include Sizzle library
to handle selectors for it. Sizzle is very popular library for handling
selectors, which also defines its own selectors like {\tt :eq()}, or {\tt
:first}. It is simpler and more expressive than CSS. Its popularity is 
mainly based on its involvement in jQuery library. 

Being so close to required structure and workflow of SOWL, InfoCram 6000 served
as the base implementation for it in the early stages. As can be seen at the end 
of this chapter, the first implementation named SelectOWL caries similar user
interface and make use of several modules of the InfoCram implementation. 


\secc Selenium

Selenium is a collection of tools for automated testing of web pages. This tools include: 

\begitems
  * Selenium IDE -- a Firefox plugin for creating test scenarios
  * WebDriver -- a set of libraries for various languages capable of running
    tests generated from Selenium scenarios
\enditems

A user of Selenium, typically a web designer, programmer or coder, would create
a scenario using Selenium IDE, in order to test his web server. From this
scenario a unit test can be generated for desired programming language and in
desired form (e.g. JUnit test case). Such a test can be simply included it in a
set of tests for the web server project. WebDriver library needed for running
these tests is available through Maven. There is also a chance to use PhantomJs
no-gui web browser for running tests without a need for actual browser, for
cases when tests are being executed automatically in background or on server
environment without X server or other form of graphical interface. The
capabilities of WebDriver make it one of the most popular testing platforms for
web servers nowadays. 

Selenium IDE is a Firefox plugin that allow us to directly record user actions
on webpage such as following links, storing and comparing values, filling in
and submitting forms. 

An attempt was made to implement SOWL as a plugin for Selenium IDE. This plugin
would have two parts: 

\begitems \style n
  * an extension of graphical interface
  * a formatter that would generate scenarios for crOWLer in some desired form
\enditems

Certain limitations were discovered during development of this plugin.
Selenium IDE, as being plugin itself, implements its own plugin system,
through which it allows other developers to extend its functionality. The
Selenium IDE plugin API allows us to use standard Firefox techniques along with
predefined API, to extend the graphical interface and the functionality of the
IDE respectively. 

Graphical interface is defined using XUL, the standard Mozilla XML format for
defining user interface. XUL defines an overlay system using which a new layer
is defined and layered over existing part of application layout while extending
or modifying it. The overlay system itself comes with Mozilla stack and can be
used on IDE by default.

\midinsert \clabel[imgseleniumide]{Image of Selenium IDE}
\picw=10cm \cinspic selenium-ide.png
\caption/f GUI of Selenium IDE showing the Command, Target and Value fields. 
\endinsert

The functionality of IDE is, however, linked to its layout
\ref[imgseleniumide] and has to be taken in account. Selenium IDE internally
defines set of commands that can be used in scenarios. List of default commands
can be seen in dropdown on main screen of the IDE. This list can be extended,
but the use and structure of commands is implemented internally in Selenium
IDE. Addition of new commands is accomplished  by extending the {\tt
Selenium.prototype} object in registered plugin. After the extension is
processed through internal command loader, a new set of commands is added for
user to use. 

Commands in this system are recognized by their names as they are assigned on
the prototype object the prefixes used are: 

\begitems
  * do -- the action commands -- for performing user actions
  * get and is -- the accessor commands -- for testing and/or waiting for a
    values on page and potentially storing it
  * assert -- the assertion commands -- for performing actual tests
\enditems

When command is generated the prefix is being stripped and according to type,
multiple versions commands can be created. For example do commands have always
``immediate'' and ``patient'' version and in this principle {\tt
Selenium.prototype.doClick} will generate the {\tt click} and {\tt
clickAndWait} command. Accessor commands are even more complex and generate
eight commands for every single method (positive and negative assertion, store
method, waitFor, etc.). Implementation of the command method defines how
Selenium IDE would behave when ``replying'' the scenario recorded. Technically it
is possible to leave the implementation empty in the IDE and use it only as a
command for WebDriver unit test. 

None of the original command types corresponds to format of commands for
handling the semantic annotation, like adding URI to element, recording
creation of individual, assigning literal to its property etc. A new set of
commands was suggested and partially implemented having the prefix ``owl''.
This led to changes in core sources of Selenium IDE, which by itself is not a good practice
as it technically creates a new branch of the program. CommandBuilder had to be
extended directly in the Selenium code as it is impossible to change its
behavior through native Selenium IDE API. Unfortunately, even though the new
command type was implemented, it is not possible to change the more general
concept of all commands. Every command is stored as {\tt (name, target, value)
}~\urlnote{https://code.google.com/p/selenium/source/browse/ide/main/src/content/commandBuilders.js
the CommandBuilder implementation} triple and from this format everything is
derived. It is technically impossible to create command for example for a
creation of an ontological literal 
along with its language tag assignment as there is simply no field for it. For the same
reason we cant create a command to create an ontological object of some type as
a property of another object.  These commands relate to each other, but such a
behavior is not supported by the scenario editor in its current architecture.
There is also no way to alter editor GUI for specific command. For instance, we
can not offer autocomplete for input field when user enters URI of ontological
resource. Such a feature would be an essential part of SOWLs workflow, and as
a consequence these limitations are critical and disallow us from properly
implementing SOWL on top of the Selenium IDE. 


\sec Libraries for SOWL

Research on existing JavaScript libraries that handle RDF data resulted in two
promising libraries: jOWL and rdfquery. Both are based on the jQuery library 
and both claim to be capable to parse RDF files, which is the main requirement
for us. Additionally the library might be used in SOWL as a storage for the 
loaded RDF resources. 


\secc jQuery

jQuery\cite[jquery] is a widely used JavaScript library that simplifies general
tasks like DOM manipulation or event handling. A simplified selectors can be
used to target DOM elements as jQuery internally uses Sizzle\cite[sizzle]
library for selector handling. Compared to Vanilla JavaScript\cite[vanillajs], jQuery
produces more compact and coherent code. 

Developers can extend the jQuery library with their own plugins. This is the
case for two most promising JavaScript libraries handling RDF and OWL data, and
so jQuery will be necessary if we decide to use either jOWL\ref[sec:jowl] or
rdfQuery\ref[sec:rdfquery]. 


\label[sec:jowl]
\secc jOWL

The jOWL library is a jQuery plugin for navigating and visualising OWL-RDFS
documents\cite[jowl]. It can parse and handle RDF files, store them in its internal
storage and query on them using subset of QUERY-DL language\cite[querydl]. 
The library was last updated in
2008~\urlnote{https://code.google.com/p/jowl-plugin/}. 


\label[sec:rdfquery]
\secc rdfQuery

rdfQuery\cite[rdfquery] is a JavaScript library for RDF-related processing. It
supports parsing RDFa, RDF, OWL formats for loading data. It can dynamically
embed HTML webpage with RDFa data. rdfQuery is written as a jQuery plugin. The
intended use of the rdfQuery library is to write queries over data stored in
rdfQuery internal datastore in similar way as DOM objects are queried using
jQuery. Moreover the whole concept is based on SPARQL and design in a manner
that make the resulting JavaScript code look familiar when compared to native
SPARQL query. 

To better show the similarity, we are presenting a rdfquery code equivalent to
this SPARQL query \ref[sec:sparql] along with printing of its output. 

\begtt
\$.rdf()
  .prefix('foaf', 'http://xmlns.com/foaf/0.1/')
  .where('?person a foaf:Person')
  .optional('?person foaf:name ?name')
  .each(function () {
    var person = this.person.value,
        name = this.name === undefined
                 ? 'Anonymous'
                 : this.name.value;
    console.log(person + " has name: " + name);
  });
\endtt


\secc aardvark

Aardvark is a JavaScript engine for in-place modifications of a webpage. It
allows user to select, delete, or highlight part of HTML page. It has been
released in two forms: as a bookmarklet and a Firefox extension. The later was
used in a modified form in the InfoCram 6000\ref[sec:infocram] and later in one
of SOWL (SelectOWL) prototypes\ref[sec:selectowl]. This library help to
implement the selection and serves as a framework for the selector generating
algorithm. 

