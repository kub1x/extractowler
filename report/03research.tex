\label[chap:research]
\chap Existing solutions % and Research

In this chapter we'll describe the research made on existing solutions for
given task (scraping and annotating data from a web). The performed search was
focused on tools directly targeting the problem, as well as libraries and
technologies that could be included in the solution or existing open source
programs we could build the solution on. 

%Semanticke a nesemanticke crowlery (linked media framework, calimachus, ...)
%je toho proste malo ;-) a proto to delame

\sec Semantic and non semantic crawlers

By researching existing solutions, there is currently no open source or openly
available solution that would directly follow the required workflow and
fulfill the requirements. 

Existing tools named as ``Ontology-based Web Crawlers'' refer mostly to crawlers
that ``rank'' pages being crawled by guess-matching them against some ontology.
In those programs user can't specify data that are being retrieved. Moreover,
there is no way to get involved in the crawling process. The tool is solely
used to automatically rank the relevance of documents. They are solving
different task where input is several documents and possibly an ontology and
output is the best matching document. 

In case we are trying to solve the input is one or more documents and one or
more ontologies and the result is data retrieved from the documents and
annotated with resources from the ontologies. 


\secc Advantages and pitfalls of Semantic crawlers

To properly target the benefits the semantification of the scraped data brings
to the user, let us quickly follow an evolution from the most primitive
technologies for scraping data to the advanced ones. The ultimate goal is to
effectively search in data and maximally utilize the knowledge it carries. 

The simplest approach is manual searching for keywords, or even simple browsing
the web. That might be useful in some cases, but when there is a lot of data,
it becomes exhausting. 

Crawling data using simple tools like {\tt wget --mirror} allows us to load data
and then write a program or script to retrieve a relevant information. This
approach takes a lot of energy for one time only solution of a given problem. 

By storing such crawled data into database we obtain persistent database,
possibly automatically obtained by the script from previous case. Such data is
static, but can be queried over and over and possibly re-retrieved when becomes
obsolete. It's structure is, however, based on programmers imagination an
needs to be described in order to understand and handle the data properly. 

When a triple store is used as the database in previous case we obtain one-time
solution to our problem. This is technically equal to original state of
crOWLer. 

When using Ontology-based solution, tailor made for crawling and annotating
data from web, we obtain several benefits ``for free''. The tool designed
specially for this purpose makes it easy. Once the data is annotated, we can
not only query on them, but also automatically reason on them and obtain more
or more specific/narrow results than with general data. The attributes and
relations within ontology, that allow reasoning, are usually part of the
ontology definition and as such comes in naturally without any extra effort. 

Last for benefits: using ontology from public resource as a schema for our data
can give us correct structure without need for building it from scratch. Also
by using some common ontology, we can join together any accessible data
structured according to this ontology and simply query on resulting super set.

~

Semantic crawling is not a silver bullet. The technology is only finding it's
place and uses and it's being shaped by the needs of it's users.

For instance there is always a threat of inconsistency of an ontology when some
data don't fit the rules or break structure of an ontology. In its state from
April 2014 DBpedia states, there is 3.64 million resources, out of which 1.83
million are classified in a consistent Ontology \cite[dh:dbpedia]. That is only 
half of the data being arguably consistent with each other. That does not say
the rest is bad. Only that it might cause a inconsistency and prevent us from
reasoning if we include a wrong subset of the data. 

Just like with ``hardcoded'' crawling technique, the semantic crawling is
tightly bound to the structure of the crawled web. The web is being matched
against some pattern described by selectors and the matching element, when
found, is accepted for further processing. Any change on a webpage structure
can lead to broken selectors or links during the crawling process and make the
scenario partially or completely invalid. 

Many web pages load their data dynamically using AJAX queries. Some pages
simply changes it's content frequently (e.g. news pages, forums, user content
pages, like video or music servers and social web applications). Crawling
content on such servers would require almost constant crawling and would cause
growth into massive ontology of, oftentimes, questionable quality. 

The semantic crawling is an useful way to effectively obtain and query on
data from the web, but it still have it's challenges to overtake. 


\sec Analysis of crOWLer

A thorough analysis of the current program shall precede creation of the final
design. We will focus on architecture, dependencies and components that will
have to be reimplemented. 

\midinsert \clabel[pic:crowler-orig]{General architecture of the original crOWLer implementation}
\picw=10cm \cinspic crowler-orig.png
\caption/f General architecture of the original crOWLer implementation
\endinsert

Original implementation is a prototype of console Java application. It uses
Apache Jena library\cite[jena] for handling ontological data and JSOUP
library\cite[jsoup] for accessing webpages and addressing elements. As a
scenario crOWLer accepts Java {\tt .class} files containing a implementation of
{\tt ConfigurationFactory} class.  In
appendix~\ref[app:crowler-old-configuration-class] you can see definition of
the configuration components of crOWLer. The class diagram in
appendix~\ref[app:crowler-old-selector-class] describes the {\tt
InitialDefinition} and the {\tt Selector} classes along with their connection
and usage of crOWLer. Configuration defined using this tructurew specifies all
the information needed for crawling process: 

\begitems
  * webpage to be crawled
  * way to address data on that page using JSOUP selectors
  * definition of ontology resources used to annotate the obtained data
  * and definition of pagination process that brings us to next page to be crawled
\enditems

XXX XXX XXX
XXX XXX XXX
XXX XXX XXX


\midinsert \clabel[pic:crowler-old-core-class]{Core classes of original crOWLer implementation} 
\picw=10cm \cinspic crowler-old-core-class.png
\caption/f Core classes of original crOWLer implementation
\endinsert


Additionally the pagination and selector implementation are supported by several
helper classes for chaining selectors or generation of a list of URL addresses
by incrementing specific argument. 

This reveals the issue being addressed. Java implemented configuration requires
knowledge of Java programming language along with knowledge of RDF
technologies. Programmer gets into the position of ontological engineer when
designing new resources. Knowledge of WEB technologies is needed in order to
properly target elements on the webpage using JSOUP selectors. This is one of
the hardest task as the selectors have to be manually extracted using for
example browser console. 

Following code is an example of actual configuration code of original crOWLer
implementation.  It uses {\tt NPU} class as simple static storage for URI's
used in our ontology. It creates a {\em monumentRecord} object for each table
row as defined by the ``initial definition''. The second part create {\em
district} object with it's label (found in third table column denoted by the
{\tt td:eq(2)} selector) and assigns it to the record using {\em hasDistrict}
object property. The {\tt conf} object holds the configuration being passed to
the actual crawler. 

\begtt
ClassSpec chObject = Factory.createClassSpec(NPU.monumnetRecord.getURI());

conf.addInitialDefinition(
       Factory.createInitialDefinition(
         chObject,
         Factory.createJSoupSelector("table tbody tr.list")));

ClassSpec sDistrict = Factory.createClassSpec(NPU.district.getURI());
chObject.addSpec(
           Factory.createOPSpec(
             Factory.createJSoupSelector("td:eq(2)"),
                                         NPU.hasDistrict.getURI(),
                                         sDistrict));
sDistrict.addSpec(true, Factory.createDPSpec(Vocabulary.RDFS_LABEL));
\endtt

Previous example more or less defines requirements on scenario for semantic
crawler. To fully satisfy the current implementation of crOWLer, we would also
have to cover following hyperlinks on a page, firing JavaScript and browser
events and functions of transforming scraped data using for example regular
expressions or key--value mapping. 

%\secc zavislosti
%
%\begitems \style O
%  * maven - apache project managing tool
%  \begitems \style o
%    * \url{https://maven.apache.org}
%    * \url{https://maven.apache.org/run-maven/index.html}
%    * \url{https://maven.apache.org/guides/mini/guide-ide-eclipse.html}
%  \enditems
%  * sesame
%  \begitems \style o
%    * \url{http://www.openrdf.org/download.jsp} ??
%  \enditems
%  * jena
%  \begitems \style o
%    * \url{https://github.com/ansell/JenaSesame} !!
%    * or \url{https://github.com/afs/JenaSesame} ??
%    * or \url{http://jena.apache.org/} ???
%    * or \url{http://sjadapter.sourceforge.net/} ????
%    * or \url{http://sourceforge.net/projects/jenasesamemodel/}
%    * might help \url{http://www.iandickinson.me.uk/articles/jena-eclipse-helloworld/}
%    * little hint \url{http://spqr.cerch.kcl.ac.uk/?page_id=130}
%    * another hit \url{http://answers.semanticweb.com/questions/20865/how-to-get-the-jena-sesame-adapter}
%    * wiki \url{https://en.wikipedia.org/wiki/Jena_(framework)}
%    * jena vs. sesame flame \url{http://answers.semanticweb.com/questions/1638/jena-vs-sesame-is-there-a-serious-complete-up-to-date-unbiased-well-informed-side-by-side-comparison-between-the-two}
%  \enditems
%\enditems


%\secc Classes of CrOWLer
%
%\begitems
%  * ImmovableHeritageConfiguration extends MonumnetConfiguration implements ConfigurationFactory 
%  \begitems
%    * implements Configuration, which is parameter for FullCrawler.run() method
%  \enditems
%  * FullCrawler
%  \begitems
%    * implements the whole crawling algorithm
%    * 
%  \enditems
%\enditems
%
%\secc Run configuration 
%
%\begtt
%crowler cz.sio2.crowler.configurations.npu.ImmovableHeritageConfiguration file results
%crowler cz.sio2.crowler.configurations.kub1x.KbxConfiguration file results
%crowler cz.sio2.crowler.configurations.parser.SeleniumConfiguration\
%         file results generated.html
%\endtt
%
%\begitems
%  * Class ImmovableHeritageConfiguration implements Configuration class. 
%  * Folder jena\_con will be created and all the rdf's will be stored in int with names derived from ontology uri
%\enditems

\sec Strigil

Strigil is a ontological scraping system developed at  Faculty of Mathematics
and Physics of the Charles University in
Prague~\urlnote{http://xrg.ksi.ms.mff.cuni.cz/software/ld/ldi.html\#strigil}.
It represents an easily configurable tool that enables one to retrieve data from textual or weak structured documents. \cite[iiwas2013:strigil]

It
consists of webserver and backend service. The webserver offers frontend for
configuring the crawling process. The backend ten follows the configuration and
handles downloading, scraping the data and storing results. Strigil strongly
focusses on the download process. Components of the backend conform in a
structure of DownloadManager, Downloaders and Proxy servers that help to
distribute the load of data being transfered. 

The frontend part serves user interface for handling ontological data on top of
a web being scraped. It internally creates it's scraping script (will be
referred to as Strigil/XML) which strongly inspired format for scenario used in
the actual implementation later in this work and will be closely analyzed in
next chapter \ref[chap:design]. 


\secc What problem does it solve?

The architecture of Strigil is tailor made for parallel processing of
documents. The installation of Strigil requires working Apache2 web server with
PHP5, Tomcat, PostgreSQL database, OpenMQ service and several other components
before the actual deployment of Strigil into the environment. The system is
designed for processing many requests on targeted server, heavy loads of data
and long running tasks. It's complicated architecture and installation process
prevents it from being effectively used in occasional simple, yet non trivial,
scraping tasks. 

Moreover it's download system fetches only the main HTML data and treats it as
static document. This way it can't properly handle dynamic content and temporal
changes in documents performed by JavaScript. 


\secc Architecture of Strigil platform

~

\midinsert \clabel[pic:strigil-overall]{Overall Architecture of Strigil}
\picw=10cm \cinspic strigil-overall.png
\caption/f Overall Architecture of Strigil
\endinsert

\midinsert \clabel[pic:strigil-data_application]{Components of Data Application part of Strigil}
\picw=10cm \cinspic strigil-data_application.png
\caption/f Components of Data Application part of Strigil 
\endinsert

\midinsert \clabel[pic:strigil-download_system]{Components of Download System part of Strigil}
\picw=10cm \cinspic strigil-download_system.png
\caption/f Components of Download System part of Strigil 
\endinsert

\midinsert \clabel[pic:strigil-deployment]{Example deployment structure of Strigil}
\picw=10cm \cinspic strigil-deployment.png
\caption/f Example deployment structure of Strigil 
\endinsert





\secc What inspiration it brings for crOWLer

XXX I tried to include Strigil/XML XXX format in SOWL, but it was XXX
ridiculous. It would bring in an unnecessary workload on string-based
serialization from native JavaScript objects into XML format. The decision was
made to rather use native JSON serialization as described in XXX chapter
implementation. This implementation is heavily inspired by the original
Strigil/XML. Moreover it attends to improve upon readability and compactness
even though it does not reach the richness of Strigil/XML format. 

%\secc Suggestions for improvements
%
% default language tag
% language as an attribute



\sec Finding platform for frontend

In order to develop appropriate tool for generating scenarios, several similar
tools were inspected for best practices, libraries, and possible extension. 

The resulted implementation is named SOWL (short for SelectOWL) and refers to
Firefox addon for creating scenarios for crOWLer. In following sections we'll
refer to SOWL as set of requirements and a envisioned expected result of this
work. The actual implementation will be covered in later chapters. 


\label[sec:infocram]
\secc InfoCram 6000 -- ExtBrain

InfoCram 6000 is part of project ExtBrain \urlnote{http://www.extbrain.net}
that is developed here on Department of Computer Science. This specific part
was implemented by Jiří Mašek and is described as ``prototype of user
interface for visual definition of extraction rules for ExtBrain Extractor''.
It's intended usage is very close to the usage of SOWL. It is an Firefox
extension that generates rules (scenario) for extractor implemented as another
part of the ExtBrain project. 

The ExtBrain extractor is implemented in JavaScript as opposed to Java in case
of crOWLer. It extracts data according to definitions by InfoCram 6000. The
result is stored in JSON format thus not carrying semantic information, but
only set of raw data in some form. 

\midinsert
\picw=7cm \cinspic infocram.png
\caption/f Main window of InfoCram 6000
\endinsert

Main part of the extension window shows a tree view with rules being edited.
This view corresponds to required structure of scenario for crOWLer. 

Interesting part it an engine for selection elements of page. It's
implementation is based on
Aardvark~\urlnote{https://addons.mozilla.org/en-US/firefox/addon/aardvark/}, a
Firefox extension that addresses this issue using mouse selection and several
keyboard commands. 

InfoCram does not use simple CSS or XPath selectors, but include Sizzle library
to handle selectors for it. Sizzle is very popular library for handling
selectors, which also defines it's own selectors like {\tt :eq()}, or {\tt
:first}. It's simpler and more expressive than CSS. It's popularity is 
mainly based on it's involvement in jQuery library. 

Being so close to required structure and workflow of SOWL, InfoCram 6000 served
as the base implementation for it in the early stages. As can be seen at the end 
of this chapter, the first implementation named SelectOWL caries similar user
interface and make use of several modules of the InfoCram implementation. 


\secc Selenium

Selenium is a collection of tools for automated testing of web pages. This tools include: 

\begitems
  * Selenium IDE -- a Firefox plugin for creating test scenarios
  * WebDriver -- a set of libraries for various languages capable of running
    tests generated from Selenium scenarios
\enditems

A user of Selenium, typically a web designer, programmer or coder, would create
a scenario using Selenium IDE, in order to test his web server. From this
scenario a unit test can be generated for desired programming language and in
desired form (e.g. JUnit test case). Such a test can be simply included it in a
set of tests for the web server project. WebDriver library needed for running
these tests is available through Maven. There is also a chance to use PhantomJs
no-gui web browser for running tests without a need for actual browser, for
cases when tests are being executed automatically in background or on server
environment without X server or other form of graphical interface. The
capabilities of WebDriver make it one of the most popular testing platforms for
web servers nowadays XXX. 

\begitems
  * IDE - \url{http://www.seleniumhq.org/projects/ide/}
  * plugins - \url{http://www.seleniumhq.org/projects/ide/plugins.jsp}
  * current commands - \url{http://release.seleniumhq.org/selenium-core/1.0.1/reference.html}
  * documentation - \url{http://docs.seleniumhq.org/docs/index.jsp}
  * extending selenium API (blog, tutorial) - \url{http://adam.goucher.ca/?s=selenium&paged=2}
  \begitems
    * randomString example - \url{http://adam.goucher.ca/?p=1348}
  \enditems
\enditems

Selenium IDE is a Firefox plugin that allow us to directly record user actions
on webpage such as following links, storing and comparing values, filling in
and submitting forms. 

An attempt was made to implement SOWL as a plugin for Selenium IDE. This plugin
would have two parts: 

\begitems \style n
  * an extension of graphical interface
  * a formatter that would generate scenarios for crOWLer in some desired form
\enditems

Certain limitations were discovered during development of this plugin.
Selenium IDE, as being plugin itself, implements it's own plugin system,
through which it allows other developers to extend it's functionality. The
Selenium IDE plugin API allows us to use standard Firefox techniques along with
predefined API, to extend the graphical interface and the functionality of the
IDE respectively. 

Graphical interface is defined using XUL, the standard Mozilla XML format for
defining user interface. XUL defines an overlay system using which a new layer
is defined and layered over existing part of application layout while extending
or modifying it. The overlay system itself comes with Mozilla stack and can be
used on IDE by default.

\midinsert \clabel[imgseleniumide]{Image of Selenium IDE}
\picw=10cm \cinspic selenium-ide.png
\caption/f GUI of Selenium IDE showing the Command, Target and Value fields. 
\endinsert

The functionality of IDE is, however, linked with it's layout
\ref[imgseleniumide] and has to be taken in account. Selenium IDE internally
defines set of commands that can be used in scenarios. List of default commands
can be seen in dropdown on main screen of the IDE. This list can be extended,
but the use and structure of commands is implemented internally in Selenium
IDE. Addition of new commands is XXX accomplished  by extending the {\tt
Selenium.prototype} object in registered plugin. After the extension is
processed through internal command loader, a new set of commands is added for
user to use. 

Commands in this system are recognized by their names as they are assigned on
the prototype object the prefixes used are: 

\begitems
  * do -- the action commands -- for performing user actions
  * get and is -- the accessor commands -- for testing and/or waiting for a
    values on page and potentially storing it
  * assert -- the assertion commands -- for performing actual tests
\enditems

When command is generated the prefix is being stripped and according to type,
multiple versions commands can be created. For example do commands have always
``immediate'' and ``patient'' version and in this principle {\tt
Selenium.prototype.doClick} will generate the {\tt click} and {\tt
clickAndWait} command. Accessor commands are even more complex and generate
eight commands for every single method (positive and negative assertion, store
method, waitFor, etc.). Implementation of the command method defines, how
Selenium IDE would behave when ``replying'' the scenario recorded. Technically it
is possible to leave the implementation empty in the IDE and use it only as a
command for WebDriver unit test. 

None of the original command types corresponds to format of commands for
handling the semantic annotation, like adding URI to element, recording
creation of individual, assigning literal to it's property etc. A new set of
commands was suggested and partially implemented having the prefix ``owl''.
This led to changes in core sources of Selenium IDE, which itself is a bad sign
as it technically creates a new branch of the program. CommandBuilder had to be
extended directly in the Selenium code as it's impossible to change it's
behavior through native Selenium IDE API. Unfortunately, even though the new
command type was implemented, it is not possible to change the more general
concept of all commands. Every command is stored as {\tt (name, target, value)
}~\urlnote{https://code.google.com/p/selenium/source/browse/ide/main/src/content/commandBuilders.js
the CommandBuilder implementation} triple and from this format everything is
derived. It is technically impossible to create command for example for literal
along with it's language tag as there is simply no field for it. For the same
reason we cant create a command to create an ontological object of some type as
a property of another object.  These commands relate to each other, but such a
behavior is not supported by the scenario editor in it's current architecture.
There is also no way to alter editor GUI for specific command. For instance, we
can't offer autocomplete for input field when user enters URI of ontological
resource. Such a feature would be an essential part of SOWL's workflow, and as
a consequence these limitations are critical and disallow us from properly
implementing SOWL on top of the Selenium IDE. 


\sec Libraries for crOWLer

\secc WebDriver

~
TODO

\secc Apache Jena

~
TODO

\sec Libraries for SOWL

\secc jQuery

jQuery\cite[jquery] is widely used JavaScript library that simplifies general
tasks like DOM manipulation or event handling. A simplified selectors can be
used to target DOM elements as jQuery internally uses Sizzle\cite[sizzle]
library for selector handling. Compared to Vanilla JavaScript\cite[vanillajs], jQuery
produces more compact and coherent code. 

Developers can extend the jQuery library with their own plugins. This is the
case for two most promising JavaScript libraries handling RDF and OWL data, and
so jQuery will be necessary if we decide to use either jOWL\ref[sec:jowl] or
rdfQuery\ref[sec:rdfquery]. 


\label[sec:jowl]
\secc jOWL

The jOWL library is a jQuery plugin for navigating and visualising OWL-RDFS
documents\cite[jowl]. It can parse and handle RDF files, store them in its internal
storage and query on them using subset of QUERY-DL language\cite[querydl]. 
The library was last updated in
2008~\urlnote{https://code.google.com/p/jowl-plugin/}. 


\label[sec:rdfquery]
\secc rdfQuery

\picw=1cm \cinspic rdfquery-logo.png

rdfQuery\cite[rdfquery] is a JavaScript library for RDF-related processing. It
supports parsing RDFa, RDF, OWL formats for loading data. It can dynamically
embed HTML webpage with RDFa data. rdfQuery is written as a jQuery plugin. The
intended use of the rdfQuery library is to write queries over data stored in
rdfQuery internal datastore in similar way as DOM objects are queried using
jQuery. Moreover the whole concept is based on SPARQL and design in a manner
that make the resulting JavaScript code look familiar when compared to native
SPARQL query. 

TODO example


\secc aardvark

Aardvark is a JavaScript engine for in-place modifications of a webpage. It
allows user to select, delete, or highlight part of HTML page. It has been
released in two forms: as a bookmarklet and a Firefox extension. The later was
used in a modified form in the InfoCram 6000\ref[sec:infocram] and later in one
of SOWL (SelectOWL) prototypes\ref[sec:selectowl].



\label[sec:selectowl]
\sec Early implementation

TODO

\midinsert
\picw=7cm \cinspic selectowl.png
\caption/f View at original SelectOWL sidebar implementation. 
\endinsert


TODO Aardvark and Infocram

The InfoCram specific functionality was carefully removed and replaced with
procedures following the SOWL's workflow. 

The added/modified features are namely: the selector creation feature, the
drag/drop event handling, the messaging, and modification of user commands. 

XXX describe each feature here or later in this chapter...

